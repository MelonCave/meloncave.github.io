
# **Information as Entropic Change: Thermodynamic Sampling For Comparison, Inference**

## **Section 1: Information as a Quantifiable Change in Knowledge**

The premise that information is a "change from what we think we know" moves the concept from a colloquial notion to a rigorous, mathematical one. In the framework established by Claude Shannon, "what we know" is not a static fact but a probabilistic landscape of possibilities. Information is the event that reshapes this landscape, and "sampling" is the mechanism through which we observe this change.

### **1.1 The Shannon Paradigm: Information as "Surprisal"**

Claude Shannon's mathematical theory of communication defines information stochastically, or probabilistically. The fundamental unit of information is not tied to semantic meaning but to the reduction of uncertainty.  
Shannon's definition of "self-information," a concept also termed "surprisal," was chosen to meet several key axioms. The information I associated with a specific event E is given by the formula I(E) \= \\log(1/p(E)) or, more simply, I(E) \= \-\\log p(E), where p(E) is the probability of that event occurring.  
This formulation is axiomatic:

1. An event with 100% probability (p(E)=1) is perfectly unsurprising and yields I(E) \= \-\\log(1) \= 0 bits of information.  
2. The less probable an event is, the more surprising it is, and the more information it yields.  
3. If two independent events are measured, the total information is the sum of their individual self-informations.

This measure is called "surprisal" because it quantifies the "surprise" of seeing an outcome. As a "post hoc measure of event expectancy" , it precisely captures the user's query. Information is only gained *after* an event (a sample) is observed, and its quantity is measured *relative to* the prior expectation (the "KNEW" state).  
A famous illustration of this distinction is the correspondence between Victor Hugo and his publisher. Following the publication of *Les Misérables*, Hugo sent a postcard with just the symbol "?". His publisher replied with a single "\!". Within their shared context, these single-symbol messages were loaded with meaning. The information was not in the *size* of the message, but in the *magnitude of the change* it produced. The "KNEW" state was "high uncertainty about the book's reception," and the "ARE" state, signaled by "\!", was "certainty of success." The single-bit message closed a vast gap of uncertainty.

### **1.2 The "KNEW" State: Formalizing Prior Knowledge as Shannon Entropy**

While surprisal measures a single event, **Shannon Entropy**, H(X), measures the *average* surprisal or *total uncertainty* of an entire system X. It is the expected value of the self-information and is defined as:  
This formula, which is formally identical to entropy in statistical mechanics , quantifies the amount of uncertainty involved in the value of a random variable.  
The user's "what we KNEW" state is not a single fact but a **probability distribution**, P\_{KNEW}(X), representing all our beliefs about a system. The *quantity* of our ignorance about this state is its entropy, H(P\_{KNEW}).  
The properties of H formalize this:

* **Maximum Entropy:** A uniform probability distribution (e.g., a fair coin flip or a fair six-sided die) yields maximum uncertainty and therefore maximum entropy. This represents a state of minimal prior knowledge.  
* **Minimum Entropy:** A state of perfect certainty (e.g., a double-headed coin that always lands heads) has no uncertainty. The entropy is zero, and no new information can be gained from a "sample".

### **1.3 The "ARE" State: Information as Uncertainty Reduction**

In this framework, the *receipt* of information is synonymous with the *reduction* of uncertainty, which is quantified as a *decrease* in entropy. When we receive data (a "sample"), we reduce the set of possibilities. As one source notes, the best research often "prune\[s\] the tree of knowledge, rather than grow\[ing\] it". By eliminating possibilities, we lower the system's entropy.  
The "ARE" state is not a single, new fact. It is a new, *updated* probability distribution, P\_{ARE}(X), which reflects the incorporation of new data. On average, the entropy of this new state is lower than the original: H(P\_{ARE}) \< H(P\_{KNEW}). The process of "sampling" is what provides the information to drive this transition.

## **Section 2: Bayesian Inference: The Engine for Updating KNEW to ARE**

If the KNEW \\to ARE transition is the goal, Bayesian inference is the engine. It provides the formal mathematical mechanism that describes how a rational agent updates their beliefs (their "KNEW" state) in the presence of new evidence (a "sample") to arrive at a new, more informed belief state (the "ARE" state).

### **2.1 Bayes' Theorem as the Formalism of Learning**

Bayes' Theorem is a rule of conditional probability that is the foundation of Bayesian statistics. The theorem is:  
This formula provides a direct mapping to the user's query, linking the degree of belief in a proposition *before* and *after* accounting for evidence :

* *The "KNEW" State (Prior):* P(H) is the **prior probability distribution**. This is the "initial degree of belief in H" , our "subjective beliefs" , or our "prior knowledge" *before* any new data is observed.  
* **The "Sample" (Likelihood):** P(D|H) is the **likelihood function**. This is the probability of observing the *sample* (the data D) *given* that our hypothesis H is true. This term formally connects the new evidence to our existing belief structure.  
* **The "ARE" State (Posterior):** P(H|D) is the **posterior probability distribution**. It represents our "updated knowledge" or the "epistemic uncertainty" about the parameters *after* taking the observed data D into account.

The posterior ("ARE") is therefore a "compromise" between the prior ("KNEW") and the likelihood (the "evidence" from the sample). The posterior from one round of updating can then serve as the prior for the next, allowing for sequential learning.

### **2.2 The First Role of "Sampling": The Sample as "Evidence"**

The query asks how "sampling" compares states. The first and most fundamental answer is that the "sample" *is the evidence itself*. The sample is the data D that is plugged into the likelihood function P(D|H), which in turn *drives* the entire update from P\_{KNEW} to P\_{ARE}.  
This is a *passive* role for the word "sampling." The "sampling distribution" P(D|H) describes the data-generating process of the world. Bayesian thinking allows for the "formal incorporation of what one knows before collecting data and then updating what is known with acquired data". In this sense, the sample *is* the "change" that Shannon's theory quantifies. This role is distinct from the *active, computational* roles of sampling that will be explored later.

## **Section 3: Measuring the Change: Information-Theoretic Distances**

Having established P\_{KNEW} (the prior) and P\_{ARE} (the posterior), we can now define a formal, quantitative measure of the "CHANGE" between these two belief states. The primary tool for this is the Kullback-Leibler Divergence.

### **3.1 Kullback-Leibler (KL) Divergence: The "Excess Surprisal" of KNEW**

The **Kullback-Leibler (KL) Divergence**, also known as **relative entropy**, is a statistical measure that quantifies how one probability distribution P diverges from a reference distribution Q.  
We map P to our "ARE" state (P\_{ARE}), which we now consider the "true" or best-informed distribution, and map Q to our "KNEW" state (P\_{KNEW}), our old approximation. The KL divergence $D\_{KL}(P\_{ARE} |  
| P\_{KNEW})$ is defined as:  
$$D\_{KL}(P\_{ARE} |  
| P\_{KNEW}) \= \\sum\_{x} P\_{ARE}(x) \\log \\frac{P\_{ARE}(x)}{P\_{KNEW}(x)}$$  
This can be rewritten in terms of expectation:  
$$D\_{KL}(P\_{ARE} |  
| P\_{KNEW}) \= E\_{x \\sim P\_{ARE}}$$  
The interpretation of this formula is the "expected excess surprisal" or "information loss". It measures, in bits (if using \\log\_2), the *extra information* we would need, on average, to encode samples from P\_{ARE} if we incorrectly used a code optimized for P\_{KNEW}.  
Therefore, $D\_{KL}(P\_{ARE} |  
| P\_{KNEW})$ is the formal, quantitative measure of "how wrong" our P\_{KNEW} state was, or equivalently, how much information we *gained* in the update.

### **3.2 The Asymmetry of Change: Why D\_{KL} is Not a "Distance"**

A critical, non-intuitive property of KL Divergence is that it is *not* a true distance metric. It is **asymmetric** , meaning:  
$$D\_{KL}(P\_{ARE} |  
| P\_{KNEW}) \\neq D\_{KL}(P\_{KNEW} | | P\_{ARE})$$  
This asymmetry is not a mathematical inconvenience; it is the *correct* formalization for a directional KNEW \\to ARE update. The measure $D\_{KL}(P\_{ARE} |  
| P\_{KNEW})$ answers the question, "Now that I am in the 'ARE' state, how much excess surprisal (or 'information gain') did I eliminate compared to my 'KNEW' state?" This is the standard measure of information gain. The reverse, D\_{KL}(P\_{KNEW} | | P\_{ARE}), answers a different, less common question: "If the 'KNEW' state were the truth, how much information would be lost by approximating it with the 'ARE' state?" The "change" is directional, and the KL divergence correctly respects this directionality.

### **3.3 Mutual Information: The *Expected* Change from Sampling**

KL Divergence is also directly related to **Information Gain**, a term used in machine learning (e.g., for building decision trees) to measure the reduction in entropy. The information gain is the reduction in uncertainty (entropy) achieved after partitioning a dataset based on an attribute: IG(S, a) \= H(S) \- H(S | a).  
The *expected* value of this information gain, averaged over all possible outcomes, is the **Mutual Information** I(X;Y). This quantity can be shown to be the *expected KL divergence* between the posterior and prior distributions :  
In the context of our KNEW \\to ARE framework, the Mutual Information I(H;D) represents the *average "change"* (in the KL sense) that we *expect* to achieve by performing our sampling, averaged over all possible samples D we could receive.

### **3.4 A Critical Nuance: When "Information" is Negative**

While the *expected* information gain (Mutual Information) is always non-negative , the information gain from a *single, specific sample* D=d can, in fact, be negative.  
This occurs when a "surprising" sample d is observed—one that strongly conflicts with the P\_{KNEW} prior. Such a "black swan" event can shatter our prior model, resulting in a posterior P\_{ARE} \= P(H|d) that is *wider* and *more uncertain* (i.e., has a higher entropy) than the prior P\_{KNEW}. This is a profound and honest feature of Bayesian reasoning. A "sample" does not always clarify; if it reveals that our foundational "KNEW" model was fundamentally wrong, the rational response is to adopt an "ARE" state of *greater* uncertainty ("I am less sure than I was before").

## **Section 4: Sampling as the Process of Inference and Comparison**

In simple cases, the P\_{KNEW} \\to P\_{ARE} update can be calculated analytically. In most complex, real-world problems (from AI to physics), it cannot. This intractability introduces the second and third roles of "sampling": not as passive *evidence*, but as an *active computational process* for performing inference and comparison.

### **4.1 The Problem of Intractability: When the "ARE" State Cannot Be Calculated**

The Bayesian update P(H|D) \\propto P(D|H)P(H) is often "analytically intractable". The bottleneck is the denominator P(D), the "evidence" or "marginal likelihood". Calculating this requires integrating over the *entire* (often high-dimensional) parameter space H:  
In high-dimensional spaces, this integral is impossible to solve. This is the "curse of dimensionality". Simple approximation methods like histograms or grids fail catastrophically, as the number of bins required grows exponentially with the number of dimensions. In these situations, "we need to resort to approximation techniques".

### **4.2 The Second Role of "Sampling": Inference via Markov Chain Monte Carlo (MCMC)**

The solution is to change the goal: if we cannot *calculate* the P\_{ARE} distribution, we will instead *draw samples from it*. This is the second role of "sampling."  
**Markov Chain Monte Carlo (MCMC)** methods are the computational engines for this task. MCMC is a class of algorithms that generates a "Markov chain"—a sequence of random samples—where the *equilibrium distribution* of the chain *is* the target posterior distribution, P\_{ARE}.  
The "clever trick" of MCMC algorithms (like the Metropolis-Hastings algorithm) is that they do not need to know the intractable P(D). They work by only computing the *ratio* of the posterior density at a new proposed state versus the current state. This ratio is:  
Since the intractable P(D) cancels out, we only need to calculate the (Prior \\times Likelihood) for the new and old states, which is computationally feasible. By running this chain for thousands of steps, we generate a large set of samples \\{x\_i\\} that are a faithful representation of the "ARE" state.

### **4.3 The Third Role of "Sampling": Comparison via Monte Carlo Approximation**

We now have our P\_{KNEW} (an analytical formula) and a large set of MCMC-generated samples \\{x\_i\\}\_{i=1...N} that *represent* P\_{ARE}. This brings us to the third role of sampling: how do we *compare* them?  
We use **Monte Carlo (MC) Approximation**. The law of large numbers states that we can approximate the expected value of any function f(X) by taking the average of f(x\_i) over a large number of samples x\_i drawn from the distribution P(X) :  
We can apply this technique to *estimate* the KL divergence, which is an expectation. Using the samples \\{x\_i\\} drawn from P\_{ARE} (via MCMC), we can approximate $D\_{KL}(P\_{ARE} |  
| P\_{KNEW})$:  
$$D\_{KL}(P\_{ARE} |  
| P\_{KNEW}) \= E\_{x \\sim P\_{ARE}} \\approx \\frac{1}{N} \\sum\_{i=1}^{N} (\\log P(x\_i | D) \- \\log P(x\_i))$$  
(Note: Estimating the P(x\_i | D) term itself can be difficult, but other approximations exist ). The principle remains: we use the samples generated by MCMC (Role 2\) as the *input* to an MC approximation (Role 3\) to *estimate* the "change" (D\_{KL}) between the "ARE" state and the "KNEW" state.

### **4.4 The Three Roles of Sampling in the KNEW-to-ARE Framework**

The user's query about "sampling" can be disambiguated into three distinct, sequential roles. This framework provides a complete answer to how sampling compares the "KNEW" and "ARE" states.

| Role | Formal Name | Function in the KNEW-to-ARE Framework |
| :---- | :---- | :---- |
| **Role 1: Evidence (The "Trigger")** | **Likelihood Function** | The *observed data* D (the "sample") that *drives* the Bayesian update from P\_{KNEW} to P\_{ARE}. This is the *input* to the change. |
| **Role 2: Inference (The "Process")** | **Markov Chain Monte Carlo (MCMC)** | The *computational algorithm* used to *generate* a large set of representative samples *from* the intractable posterior P\_{ARE} when it cannot be analytically solved. |
| **Role 3: Comparison (The "Tool")** | **Monte Carlo (MC) Approximation** | The *mathematical technique* used to *estimate* the quantitative change (e.g., D\_{KL}) by averaging over the samples generated by MCMC. |

## **Section 5: Sequential Sampling and the Detection of Change**

The framework developed thus far—a KNEW \\to ARE update quantified by D\_{KL}—can be extended from a single update to a *dynamic, time-series* context. Here, we are not just making one update, but continuously monitoring a stream of samples to determine *when* the underlying P\_{KNEW} state has *fundamentally changed* to a new P\_{ARE} state.

### **5.1 The Classical View: Hypothesis Testing**

The classical, frequentist approach to this problem is **Hypothesis Testing**.

* The **"KNEW"** state is the **Null Hypothesis (H\_0)**, which states there is no effect or no difference.  
* The **"ARE"** state is the **Alternative Hypothesis (H\_a)**, which states there *is* a real effect.

"Sampling" is the process of collecting data. The test determines the probability (the *p-value*) of obtaining the observed sample data *if the null hypothesis (H\_0) were true*. If this probability is very low (typically \< 0.05), the result is "statistically significant," and we *reject* the "KNEW" state (H\_0) in favor of the "ARE" state (H\_a). This is a binary decision framework.

### **5.2 The Dynamic View: Change Point Detection (CPD)**

A more direct application for a *stream* of samples is **Change Point Detection (CPD)**. CPD algorithms are designed to monitor sequential data and identify the *time step* at which "abrupt variations" or "transitions between states" occur.  
A "change point" is the exact moment k where the data-generating distribution shifts from P\_{KNEW} (for t \< k) to a new P\_{ARE} (for t \\ge k). These methods are "sequential" and often "online" , designed to detect this change in real-time, as quickly as possible, while maintaining a low false alarm rate.

### **5.3 The KL Divergence as the *Driver* of Detection Speed**

This dynamic view provides a powerful synthesis of the entire framework. A central question in sequential analysis is: how many samples does it take to *detect* the change? This "detection delay" is not arbitrary.  
Recent analysis reveals that the **detection delay is asymptotically inversely proportional to the Kullback-Leibler Divergence** between the pre-change (P\_{KNEW}) and post-change (P\_{ARE}) distributions.  
This is a critical unification. The D\_{KL} which Section 3 established as the *abstract measure* of the "change" is here revealed to be the *fundamental physical property* that governs the *detectability* of that change.

* If the change is massive (a large D\_{KL}), the "ARE" state is very different from the "KNEW" state. The "excess surprisal" of new samples is high, and the change is detected very quickly.  
* If the change is subtle (a small D\_{KL}), the two distributions are very similar. It will take a large number of samples to statistically distinguish them, and the detection delay will be long.

## **Section 6: Physical Dimensions: The Thermodynamics of Updating Knowledge**

The KNEW-to-ARE framework is not merely a statistical abstraction. The "change" that Shannon defined as information is a *physical* process, grounded in the laws of thermodynamics and quantum mechanics. The update from P\_{KNEW} to P\_{ARE} is a physical computation with real-world costs.

### **6.1 The Equivalence of Information and Thermodynamic Entropy**

The H in Shannon's entropy formula is formally identical to the H in Boltzmann's famous H-theorem from statistical mechanics.

* In thermodynamics, a **macrostate** (our "KNEW" state) describes large-scale properties we can measure (e.g., pressure, temperature, volume).  
* A **microstate** is the precise configuration of all individual particles (e.g., their positions and momenta).  
* **Thermodynamic Entropy (H)** is a measure of our *ignorance* about the true microstate, given that we only know the macrostate. It quantifies the number of microstates that are consistent with our macro-level knowledge.

Receiving "information" (a "sample" or measurement) is any process that reduces our ignorance of the true microstate, thereby reducing the system's entropy.

### **6.2 Landauer's Principle: The Physical Cost of Forgetting "KNEW"**

This connection forms the basis of the **thermodynamics of computation**. This field investigates the fundamental energy costs of information processing.  
The key finding is **Landauer's Principle**, which states that any *logically irreversible* computation has a minimum, unavoidable thermodynamic cost. The canonical example is *erasing a bit of information*. This act must dissipate at least kT \\ln 2 of energy as heat into the environment, where k is the Boltzmann constant and T is the temperature.  
The KNEW \\to ARE update is a logically irreversible computation. To adopt the new P\_{ARE} state, one must *erase* or *discard* the old P\_{KNEW} state. This act of "forgetting" is physically costly. This is not a metaphor. The human brain, which is a massive Bayesian inference engine, consumes \~20% of the body's total calories while "doing nothing but compute". This massive metabolic cost is the thermodynamic price of a system that is continuously running KNEW-to-ARE updates—that is, *learning*.

### **6.3 Thermodynamic Computing: Hardware as a Sampling Engine**

This physical reality is now being used to design new forms of hardware. A **Thermodynamic Sampling Unit (TSU)** is a "probabilistic computer" designed from the ground up to *natively sample from probability distributions*.  
This hardware directly implements the physics described in this report:

* It uses **Energy-Based Models (EBMs)**, where the probability of any state is P \\propto e^{-\\text{Energy}}. This is a direct implementation of the Boltzmann distribution.  
* The "KNEW" state is the *programmed energy landscape* on the chip.  
* The "sampling" process is an algorithm like **Gibbs sampling** , which is a physical, thermodynamic process of the hardware *settling* into its low-energy (high-probability) states.  
* The "ARE" state is the *output sample* (a '1' or '0') read from the device.

The TSU is a physical KNEW-to-ARE machine that computes by *physically sampling* from a thermodynamic model. This is also true of other physical computing paradigms, such as those that use thermal or spectral data as information carriers.

### **6.4 The Quantum "KNEW": Superposition and the Measurement "Sample"**

The deepest, most fundamental level of the KNEW \\to ARE transition is found in quantum mechanics.

* The **"KNEW"** state is the **wavefunction**, \\psi. It exists as a *superposition* of all possible states, representing the ultimate probabilistic description of a system.  
* The **"sampling"** is the *act of measurement*.  
* The **"ARE"** state is the *single, definite outcome* that is observed *after* the wavefunction "collapses".

The **Measurement Problem** in quantum physics *is* the user's query at the most fundamental level of reality: *How* does the probabilistic P\_{KNEW} (the superposition) *change* into the definite P\_{ARE} (the single outcome) via the act of "sampling" (measurement)?  
This physical property is what quantum computers seek to exploit. By manipulating wavefunctions (probabilistic "KNEW" states), they can perform computations. Models like **Quantum Boltzmann Machines** and quantum Bayesian networks aim to use quantum superposition and entanglement to *sample* from "ARE" distributions that are intractably complex for any classical computer.

## **Section 7: Synthesis and Conclusion**

The query—"How might we think about sampling to compare states between what we KNEW and the state as things now ARE?"—unifies a vast range of concepts across statistics, computer science, and physics. The answer is that "sampling" is not a single concept but a multi-faceted mechanism that *triggers*, *executes*, and *quantifies* the "change" that Shannon defined as information.

### **7.1 Recapitulation: The Three-Fold Role of Sampling**

The analysis provides a clear, three-part framework for "thinking about sampling":

1. **Sampling as Evidence (The Trigger):** The sample is the *data* D from the world. It is the *input* to the likelihood function in Bayes' Theorem, P(D|H), which *triggers* the entire update from the "KNEW" state (P\_{Prior}) to the "ARE" state (P\_{Posterior}).  
2. **Sampling as Inference (The Process):** When the "ARE" state is too complex to calculate, "sampling" (via MCMC) becomes the *computational process* we use to *generate* a large set of representative data points *from* that "ARE" state.  
3. **Sampling as Comparison (The Tool):** To quantify the "change" (D\_{KL}) between the analytical P\_{KNEW} and the sampled P\_{ARE}, "sampling" (via MC approximation) is the *mathematical tool* we use to *estimate* this distance by averaging over the samples generated in the previous step.

### **7.2 The KNEW-to-ARE Transition as a Physical, Thermodynamic Event**

Ultimately, the statement "information is a CHANGE" is not a metaphor. It is a description of a physical, quantifiable event.

* The KNEW \\to ARE transition is a formal update of a probabilistic belief state.  
* This update is mathematically quantified by the Kullback-Leibler Divergence (D\_{KL}), which measures the "expected excess surprisal" of the old model.  
* The magnitude of this D\_{KL} is not just an abstract number; it is a critical parameter that determines how *quickly* this change can be detected by a sequential sampling algorithm.  
* This abstract process of information change is a *physical process* with a *thermodynamic cost*, defined by Landauer's Principle. It is the energy cost of *erasing* the "KNEW" state to make room for the "ARE" state.

This single KNEW \\to ARE framework, powered by "sampling," is a universal process. It is implemented in our neurobiology as we learn, in our most advanced AI (like TSUs) as they infer, and it is the fundamental mystery of reality itself in the act of quantum measurement.

#### **Works cited**

1\. Information Theory \- Bits and Binary Digits, https://cs.stanford.edu/people/eroberts/courses/soco/projects/1999-00/information-theory/information\_1.html 2\. Information theory \- Wikipedia, https://en.wikipedia.org/wiki/Information\_theory 3\. Information content \- Wikipedia, https://en.wikipedia.org/wiki/Information\_content 4\. Entropy (information theory) \- Wikipedia, https://en.wikipedia.org/wiki/Entropy\_(information\_theory) 5\. What's Surprising About Surprisal \- PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC12125142/ 6\. Information theory | Definition, History, Examples, & Facts \- Britannica, https://www.britannica.com/science/information-theory 7\. A Mathematical Theory of Communication, https://people.math.harvard.edu/\~ctm/home/text/others/shannon/entropy/entropy.pdf 8\. How Claude Shannon Invented the Future | Quanta Magazine, https://www.quantamagazine.org/how-claude-shannons-information-theory-invented-the-future-20201222/ 9\. Bayes' Theorem: What It Is, Formula, and Examples \- Investopedia, https://www.investopedia.com/terms/b/bayes-theorem.asp 10\. Bayes' theorem \- Wikipedia, https://en.wikipedia.org/wiki/Bayes%27\_theorem 11\. Bayes and the Law \- PMC \- PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC4934658/ 12\. Bayesian inference \- Wikipedia, https://en.wikipedia.org/wiki/Bayesian\_inference 13\. Help me understand Bayesian prior and posterior distributions \- Cross Validated, https://stats.stackexchange.com/questions/58564/help-me-understand-bayesian-prior-and-posterior-distributions 14\. A Gentle Introduction to Bayesian Analysis: Applications to ..., https://pmc.ncbi.nlm.nih.gov/articles/PMC4158865/ 15\. A Tutorial on Modern Bayesian Methods in Clinical Trials \- PMC \- NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC10117244/ 16\. Posterior probability \- Wikipedia, https://en.wikipedia.org/wiki/Posterior\_probability 17\. Understanding KL Divergence | Towards Data Science, https://towardsdatascience.com/understanding-kl-divergence-f3ddc8dff254/ 18\. Kullback–Leibler divergence \- Wikipedia, https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler\_divergence 19\. Shannon Entropy and Kullback-Leibler Divergence \- Statistics & Data Science, https://www.stat.cmu.edu/\~cshalizi/754/2006/notes/lecture-28.pdf 20\. Kullback-Leibler Divergence Explained — Count Bayesie, https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained 21\. KL-Divergence Explained: Intuition, Formula, and Examples \- DataCamp, https://www.datacamp.com/tutorial/kl-divergence 22\. Intuition on the Kullback–Leibler (KL) Divergence \- Cross Validated \- Stats StackExchange, https://stats.stackexchange.com/questions/188903/intuition-on-the-kullback-leibler-kl-divergence 23\. Information gain (decision tree) \- Wikipedia, https://en.wikipedia.org/wiki/Information\_gain\_(decision\_tree) 24\. Information Gain and Mutual Information for Machine Learning \- GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/information-gain-and-mutual-information-for-machine-learning/ 25\. Information Gain and Mutual Information for Machine Learning \- MachineLearningMastery.com, https://machinelearningmastery.com/information-gain-and-mutual-information/ 26\. Information Gain, https://homes.cs.washington.edu/\~shapiro/EE596/notes/InfoGain.pdf 27\. A Note About Negative Information | Todd Gureckis, https://todd.gureckislab.org/2021/05/05/negative-information 28\. Importance Sampling Schemes for Evidence Approximation in Mixture Models \- Project Euclid, https://projecteuclid.org/journals/bayesian-analysis/volume-11/issue-2/Importance-Sampling-Schemes-for-Evidence-Approximation-in-Mixture-Models/10.1214/15-BA970.full 29\. \[D\] Making sense of probability in high dimensions : r/MachineLearning \- Reddit, https://www.reddit.com/r/MachineLearning/comments/fe8c51/d\_making\_sense\_of\_probability\_in\_high\_dimensions/ 30\. Is there a fast way to compute histograms for high-dimensional large datasets?, https://scicomp.stackexchange.com/questions/7572/is-there-a-fast-way-to-compute-histograms-for-high-dimensional-large-datasets 31\. Markov chain Monte Carlo \- Wikipedia, https://en.wikipedia.org/wiki/Markov\_chain\_Monte\_Carlo 32\. Stat 3701 Lecture Notes: Bayesian Inference via Markov Chain Monte Carlo (MCMC), https://www.stat.umn.edu/geyer/3701/notes/mcmc-bayes.html 33\. A simple introduction to Markov Chain Monte–Carlo sampling \- PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC5862921/ 34\. Understanding the Monte Carlo Markov Chain: A Key to Bayesian Inference, https://charlescopley.medium.com/understanding-the-monte-carlo-markov-chain-a-key-to-bayesian-inference-163b03f9fd2d 35\. Monte Carlo method \- Wikipedia, https://en.wikipedia.org/wiki/Monte\_Carlo\_method 36\. A Gentle Introduction to Monte Carlo Sampling for Probability \- Machine Learning Mastery, https://machinelearningmastery.com/monte-carlo-sampling-for-probability/ 37\. Chapter 5: Monte Carlo Approximation, https://jwmi.github.io/BMS/chapter5-monte-carlo.pdf 38\. Approximating the KL-Divergence | Two Ways in TensorFlow Probability \- YouTube, https://www.youtube.com/watch?v=x9StQ8RZ0ag 39\. Approximating KL Divergence \- John Schulman, http://joschu.net/blog/kl-approx.html 40\. Sampling and hypothesis testing | Advanced R Programming Class Notes \- Fiveable, https://fiveable.me/introduction-to-advanced-programming-in-r/unit-6/sampling-hypothesis-testing/study-guide/hCnH8BvvqUnLLVxi 41\. An Easy Introduction to Statistical Significance (With Examples) \- Scribbr, https://www.scribbr.com/statistics/statistical-significance/ 42\. The nuts and bolts of hypothesis testing \- PMC \- NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC6668281/ 43\. Sampling Methods | Types, Techniques & Examples \- Scribbr, https://www.scribbr.com/methodology/sampling-methods/ 44\. Statistical Significance \- StatPearls \- NCBI Bookshelf \- NIH, https://www.ncbi.nlm.nih.gov/books/NBK459346/ 45\. Statistical Significance: What It Is, How It Works, and Examples \- Investopedia, https://www.investopedia.com/terms/s/statistically\_significant.asp 46\. A Survey of Methods for Time Series Change Point Detection, https://eecs.wsu.edu/\~cook/pubs/kais16.2.pdf 47\. How Change Point Detection works—ArcGIS Pro | Documentation, https://pro.arcgis.com/en/pro-app/latest/tool-reference/space-time-pattern-mining/how-change-point-detection-works.htm 48\. Optimal Parallel Sequential Change Detection under Generalized Performance Measures \- LSE Research Online, https://eprints.lse.ac.uk/118348/1/Optimal\_Parallel\_Sequential\_Change\_Detection\_under.pdf 49\. \[2502.05377\] High-Dimensional Sequential Change Detection \- arXiv, https://arxiv.org/abs/2502.05377 50\. 3.2: Thermodynamic Systems \- Physics LibreTexts, https://phys.libretexts.org/Bookshelves/University\_Physics/University\_Physics\_(OpenStax)/University\_Physics\_II\_-\_Thermodynamics\_Electricity\_and\_Magnetism\_(OpenStax)/03%3A\_The\_First\_Law\_of\_Thermodynamics/3.02%3A\_Thermodynamic\_Systems 51\. Thermodynamics, https://www.grc.nasa.gov/www/k-12/airplane/thermo.html 52\. Projects: The thermodynamics of computation | Santa Fe Institute, https://www.santafe.edu/research/projects/thermodynamics-computation 53\. Revisiting thermodynamics in computation and information theory \- arXiv, https://arxiv.org/html/2102.09981v2 54\. Information Theory and Computational Thermodynamics: Lessons for Biology from Physics, https://www.mdpi.com/2078-2489/3/4/739 55\. TSU 101: An Entirely New Type of Computing Hardware | Extropic, http://extropic.ai/writing/tsu-101-an-entirely-new-type-of-computing-hardware 56\. Thermal Sensors and Satellites \- Cal Poly Humboldt Geospatial Curriculum, https://gsp.humboldt.edu/olm/Courses/GSP\_216/lessons/thermal/sensors.html 57\. An introduction to thermal infrared \- UP42, https://up42.com/blog/introduction-to-thermal-infrared 58\. Processing of multispectral thermal IR data for geologic applications \- NASA Technical Reports Server (NTRS), https://ntrs.nasa.gov/citations/19800008391 59\. Measurement problem \- Wikipedia, https://en.wikipedia.org/wiki/Measurement\_problem 60\. Comparing classical probability vs. quantum, and QP in human decision making & machine learning Camille Crumpton \- UTK-EECS, https://web.eecs.utk.edu/\~bmaclenn/Classes/494-594-UC-F16/presentations/QPpresentation.pdf 61\. The Quantum Measurement Problem: A Review of Recent Trends \- arXiv, https://arxiv.org/html/2502.19278v2 62\. Sampling problems on a Quantum Computer \- arXiv, https://arxiv.org/html/2402.16341v1 63\. \[1601.02036\] Quantum Boltzmann Machine \- arXiv, https://arxiv.org/abs/1601.02036 64\. \[quant-ph/9706039\] Quantum Bayesian Nets \- arXiv, https://arxiv.org/abs/quant-ph/9706039 65\. \[2107.09599\] Quantum Bayesian Neural Networks \- arXiv, https://arxiv.org/abs/2107.09599