The process of compressing tons of lecture content and piles of academic papers into a knowledge graph involves automatically extracting entities as nodes using [named entity recognition (NER)](https://en.wikipedia.org/wiki/Named-entity_recognition), possibly guided by an ontology or possibly letting the ontology express itself, though NER processing is just one part of a broader pipeline that includes basic segmentation and chunking, relation extraction and ontology choice finished off with graph construction to get indexed material with a meaningful structure ... so we use or don't [libraries like spaCy](https://spacy.io/usage/spacy-101#what-spacy-isnt) to monkey with the processes, frameworks, services that we'd use to put massive loads of raw technical text into the hopper and get back a coherent document outlining the context with a variety of useful annotations.

**Key Points:**
- **Core Role of NER**: It seems likely that NER, when combined with iterative ontology exploration, enables efficient identification of key entities like concepts, terms, or figures from unstructured text, forming the foundational nodes of a knowledge graph.
- **Ontology Guidance**: Evidence leans toward ontologies providing predefined categories to classify entities accurately, reducing ambiguity in diverse educational materials.
- **Automation Necessity**: For large volumes of lectures and papers, manual extraction is impractical; automated methods balance scalability with potential inaccuracies, acknowledging debates around model biases and domain adaptation.
- **Broader Compression Benefits**: This approach may facilitate summarization by interconnecting ideas, though complexities like context loss highlight the need for careful implementation across stakeholders' views.
- **Challenges and Alternatives**: While effective, NER-ontology combos can overlook nuanced relations; integrating large language models (LLMs) offers flexibility but raises concerns about reliability in sensitive academic contexts.

### Understanding Knowledge Graphs in Education
Knowledge graphs represent information as interconnected nodes (entities) and edges (relationships), compressing sprawling content like lecture transcripts or research papers into queryable structures. For instance, a graph might link "machine learning" (node) to "supervised learning" (related node) via an "includes" edge, derived from lecture notes.

### The Extraction Process
Start with preprocessing text (e.g., cleaning transcripts), then apply NER to pull entities. An ontology defines entity types, ensuring relevance—e.g., categorizing "BERT" as a "model" in NLP lectures. Tools like spaCy or GLiNER automate this, but relation extraction follows to add edges, completing the graph.

### Applications and Tools
In academic settings, this method aids in creating personalized learning paths or research overviews. Open-source libraries (e.g., Neo4j for storage) and LLMs (e.g., GPT variants) enhance accessibility, though ethical considerations around data privacy are paramount.

---
Compressing educational content such as lecture transcripts, notes, and academic papers into a knowledge graph (KG) is a multifaceted process aimed at distilling vast, unstructured information into a structured, interconnected representation that facilitates efficient querying, analysis, and knowledge discovery. The original statement emphasizes the automatic extraction of KG nodes using named entity recognition (NER) guided by an ontology, which is indeed a critical step but represents only part of a comprehensive pipeline. This expansion explores the rationale, detailed methodologies, tools, benefits, challenges, and real-world examples, drawing from established practices in natural language processing (NLP) and information extraction. By treating lectures and papers as unstructured text sources—often transcribed or digitized—the process enables "compression" by eliminating redundancy, highlighting key concepts, and revealing hidden relationships, ultimately transforming raw material into a navigable semantic network.

#### Fundamentals: Key Concepts
A **knowledge graph** is a graph-based data structure where nodes represent entities (e.g., concepts, people, objects) and edges denote relationships between them (e.g., "defines," "relates to," "cites"). In educational contexts, KGs compress content by abstracting essential knowledge, making it easier to navigate complex topics like cybersecurity or machine learning without sifting through full texts. For example, a KG from a lecture on AI might have nodes like "Neural Network" connected to "Backpropagation" via an "uses" edge.

**Named Entity Recognition (NER)** is an NLP technique that identifies and classifies named entities in text into predefined categories, such as persons, organizations, locations, or domain-specific terms (e.g., "algorithm," "theorem"). Traditional NER relies on rule-based or machine learning models, while modern approaches incorporate deep learning (e.g., BiLSTM-CRF or Transformers like BERT) for higher accuracy. In the context of KG construction, NER extracts potential nodes by scanning text for salient entities, automating what would otherwise be a labor-intensive manual process.

An **ontology** is a formal specification of concepts, properties, and relationships within a domain, acting as a schema to guide NER. It ensures extracted entities are categorized consistently—e.g., in a computer science ontology, "BERT" might be classified as a "Pre-trained Language Model." Ontologies like the Computer Science Ontology (CSO) or domain-specific ones (e.g., for cybersecurity) provide the "NER ontology" referenced in the statement, enabling ontology-based NER where entity types are predefined to align with the KG's structure.

The "compression" aspect refers to reducing information overload by structuring content into a graph, where redundant or peripheral details are filtered, and core knowledge is interconnected for quick retrieval.

#### Rationale for Automatic Node Extraction Using NER and Ontology
Manually building KGs from lectures and papers is infeasible due to volume—e.g., a single course might include dozens of hours of transcripts and hundreds of papers. Automation via NER addresses this by scaling entity extraction, while an ontology mitigates ambiguity (e.g., distinguishing "Apple" as a company vs. fruit based on context). This is essential for "compression" as it identifies core nodes efficiently, forming the KG's backbone. Without ontology guidance, NER might extract irrelevant or misclassified entities, leading to noisy graphs. Research shows that ontology-driven NER improves precision by 10-20% in domain-specific tasks, making it indispensable for educational content where terminology is specialized.

#### Detailed Pipeline for KG Construction
The full process extends beyond node extraction to include preprocessing, relation extraction, and graph assembly. Here's a step-by-step breakdown, adaptable to lecture transcripts (e.g., audio-to-text conversions) or paper PDFs:

1. **Data Collection and Preprocessing**: Gather content—e.g., transcribe lectures using tools like Otter.ai or extract text from papers via PDF parsers. Clean data by removing noise (e.g., stopwords, punctuation) and segment into chunks (e.g., sentences or paragraphs) to handle large inputs. For lectures, this might involve handling spoken language quirks like filler words.

2. **Coreference Resolution**: Resolve pronouns and references (e.g., "it" referring to "algorithm") to ensure consistent entity representation. Tools like spaCy's coreference model prevent fragmented nodes.

3. **Ontology-Guided NER for Node Extraction**: Apply NER using an ontology to identify and classify entities as nodes. For zero-shot scenarios, use models like GLiNER, which extracts entities based on ontology classes without training. Example: In a machine learning lecture, NER might extract "Supervised Learning" as a "Concept" node. Custom ontologies (e.g., via OWL format) define categories, and LLMs like GPT-4 can refine proposals. For papers, domain ontologies like Biolink (for biology) guide extraction from abstracts.

4. **Entity Linking and Disambiguation**: Link extracted entities to external knowledge bases (e.g., Wikidata) to standardize them, avoiding duplicates. Ontology-based verification checks compatibility (e.g., via SPARQL queries).

5. **Relationship Extraction**: Identify edges between nodes using rule-based methods (e.g., dependency parsing) or ML models (e.g., REBEL). For example, "Backpropagation trains Neural Networks" yields an edge "trains."

6. **KG Construction and Storage**: Populate the graph with nodes and edges, using databases like Neo4j. Enrich with attributes (e.g., definitions from ontologies) and serialize (e.g., RDF format).

7. **Validation and Enrichment**: Human review or ML-based link prediction refines the KG, adding inferred relations.

| Step | Tools/Methods | Example from Lectures | Example from Papers |
|------|---------------|-----------------------|---------------------|
| Preprocessing | spaCy, Recursive Splitter | Clean transcript: "Um, neural nets are..." → "Neural nets are..." | Extract abstract text from PDF |
| Coreference | spaCy Coref | "It learns from data" → "Neural Network learns from data" | Resolve "the model" to "BERT" in methods section |
| NER with Ontology | GLiNER, BERT-based | Extract "Gradient Descent" as "Algorithm" | Identify "Cytokine" as "Protein" using Biolink ontology |
| Entity Linking | Wikidata API, FAISS | Link "AI" to Wikidata Q11568 | Map "COVID-19" to ontology term for consistency |
| Relation Extraction | REBEL, Dependency Parsing | "Teaches" edge between "Professor" and "Topic" | "Cites" edge between papers |
| KG Storage | Neo4j, RDFlib | Graph query: "Related concepts to AI?" | Visualize citation networks |

#### Benefits in Educational Contexts
- **Summarization and Accessibility**: KGs compress lectures into visual maps, aiding students in grasping overviews—e.g., a cybersecurity KG linking "Vulnerability" to "Exploit" tools.
- **Personalized Learning**: Query graphs for recommendations, like suggesting related papers based on lecture nodes.
- **Research Efficiency**: For papers, KGs reveal trends, e.g., entity co-occurrences in scholarly graphs.
- **Interoperability**: Ontologies enable merging KGs from multiple sources, fostering collaborative education.

#### Challenges and Limitations
- **Accuracy Issues**: NER can miss context-specific entities (e.g., acronyms in lectures), with error rates up to 20% in noisy text. Ontologies may not cover emerging terms, requiring updates.
- **Scalability**: Processing large corpora demands computational resources; LLMs help but introduce biases.
- **Domain Adaptation**: General models underperform on specialized fields like cybersecurity, necessitating fine-tuning or hybrid human-AI approaches.
- **Ethical Considerations**: Automation might overlook nuanced interpretations, and privacy concerns arise with transcribed lectures.

#### Real-World Examples and Tools
- **Scholarly KGs from Papers**: Using SciBERT and CSO ontology, tools like SciREX extract entities from abstracts to build graphs for trend analysis.
- **Cybersecurity Education**: From lecture notes and labs, NER extracts entities like "IDS," linked via custom ontology to create interactive graphs for student queries.
- **Tools**: spaCy for NER, GLiNER for zero-shot ontology-based extraction, LLMs (e.g., GPT) for refinement, Neo4j for visualization. Domain-specific pipelines like OmEGa use LLMs with ontologies for manufacturing docs, adaptable to education.

In summary, while NER with ontology is pivotal for node extraction, the full pipeline ensures robust KG compression, balancing automation with domain expertise for educational value.

**Key Citations:**
- [From Text to Knowledge: How Named Entity Recognition Powers Knowledge Graphs](https://medium.com/wpp-ai-research-labs/from-text-to-knowledge-how-named-entity-recognition-powers-knowledge-graphs-b288ec375ae0)
- [From Text to a Knowledge Graph: The Information Extraction Pipeline](https://neo4j.com/blog/genai/text-to-knowledge-graph-information-extraction-pipeline/)
- [NER in Knowledge Graphs for Data Analytics & Semantic Understanding](https://ubiai.tools/integrating-ner-with-knowledge-graphs-for-advanced-data-analytics-and-semantic-understanding/)
- [Creating knowledge graphs from unstructured text](https://fairplus.github.io/the-fair-cookbook/content/recipes/interoperability/creating-knowledge-graph-from-unstructured-text.html)
- [OnNER: An Ontology for Semantic Representation of Named Entities](https://www.utwente.nl/en/eemcs/fois2024/resources/papers/reza-hahmann-onner.pdf)
- [A Brief History of Named Entity Recognition](https://arxiv.org/html/2411.05057v1)
- [Scholarly knowledge graphs through structuring scholarly communication](https://pmc.ncbi.nlm.nih.gov/articles/PMC9361271/)
- [Building Knowledge Graphs from Unstructured Texts](https://par.nsf.gov/servlets/purl/10401615)
- [Unstructured text to knowledge graph using an ontology](https://www.contentstack.com/blog/engineering/unstructured-text-to-knowledge-graph-using-an-ontology)
- [Domain Ontology-Driven Knowledge Graph Generation from Text](https://dl.acm.org/doi/10.1145/3708478)
- [A Two-Step Knowledge Extraction Pipeline with Ontology-Based Entity Linking](https://aclanthology.org/2024.textgraphs-1.5.pdf)

---
# **Knowledge Graph Construction from Lectures and Literature**

The contemporary academic landscape is characterized not by a scarcity of information, but by a paralyzing superabundance. The digitization of the university, accelerated by global shifts toward remote instruction, has resulted in an exponential proliferation of recorded lectures, preprint manuscripts, seminar transcripts, and supplementary multimodal educational materials. This data deluge presents a fundamental "epistemological crisis" for the learner: the cognitive capacity to absorb, synthesize, and retain information is biologically capped, while the volume of available information scales geometrically. In this context, the construction of Educational Knowledge Graphs (EduKGs) is not merely a technical exercise in database management; it is a critical intervention in **semantic compression**.

Semantic compression differs fundamentally from traditional data compression. While the latter seeks to reduce the bit rate while preserving the fidelity of the signal (e.g., maintaining the pixel integrity of a video), semantic compression aims to reduce the cognitive load while preserving the fidelity of the *meaning*.1 It is a lossy process that intentionally discards the "surface structure"—the specific lexical choices, the disfluencies of speech, the redundancy of phrasing—to reveal and preserve the "deep structure," or the logical propositions and causal relationships that constitute knowledge.3

### **1.1 The Lattice Theory of Information: A Mathematical Framework for Abstraction**

To rigorize the concept of semantic compression, we turn to the **lattice theory of information**, a framework that extends Shannon's classical information theory into the domain of structural abstraction. Classical theory quantifies information in bits based on probability distributions; however, it struggles to quantify the "value" or "meaning" of that information relative to a goal. Lattice theory, by contrast, models information as a partially ordered set (a lattice) of partitions on the data space.1

In the context of an academic lecture, the raw transcript represents the bottom of the lattice—the most granular, least abstract representation. As we move up the lattice, we perform abstraction operations that group distinct data points into broader categories. For instance, the specific utterances "the derivative of sine is cosine" and "the rate of change of position is velocity" can both be abstracted under the node "Calculus Operations." This hierarchical structure implies the **successive refinement property**, where coarse-grained summaries (high on the lattice) can be progressively refined into finer-grained realizations (low on the lattice) as needed for progressive transmission or learning.1

This theoretical perspective justifies the use of Knowledge Graphs (KGs) over simple vector embeddings. Vector spaces are continuous and high-dimensional, making them excellent for capturing "fuzzy" semantic similarity but poor at representing the precise, hierarchical abstractions defined by lattice theory. A graph, with its discrete nodes and edges, naturally maps to the lattice structure of knowledge. The "Information Lattice Learning" approach demonstrates that AI systems can learn to compress data into these lattice structures, thereby optimizing group codes for semantic transmission.4 By organizing lecture content into a graph, we are essentially constructing an externalized information lattice that allows the learner to traverse from high-level abstraction to low-level detail without the cognitive overhead of processing the raw signal.

### **1.2 Semantic Entropy and the Rate-Distortion Trade-off**

A central challenge in constructing these graphs is managing **semantic entropy**. In semantic information theory, entropy is not just a measure of unpredictability, but a measure of the uncertainty regarding the *truth value* of propositions within a specific context.6 When compressing a lecture into a graph, we face a "semantic rate-distortion" problem:

* **Rate:** The size of the graph (number of nodes and edges).  
* **Distortion:** The deviation of the graph's inferential capacity from the original source material.

If the compression rate is too high (i.e., the graph is too sparse), the semantic distortion increases—critical nuances, conditions, and exceptions mentioned in the lecture are lost. If the rate is too low (i.e., the graph retains too much detail), the utility of the compression is negated, and the user is overwhelmed by a "hairball" of connections that fails to provide structural clarity.7 The goal of an effective EduKG pipeline is to locate the optimal point on this rate-distortion curve, often by employing **semantic expansion** techniques during the extraction phase to ensure that the compressed representations (the nodes) remain semantically rich and unambiguous.8

This requires a "semantic distortion function" that specifically measures the loss of *educational utility* rather than just signal fidelity. Recent research into human memory supports this approach: studies show that while memory for "surface structure" (exact wording) decays rapidly, memory for "semantic structure" (the gist or mental model) is far more stable over time.3 Thus, a system designed for semantic compression must prioritize the preservation of the propositional content that constitutes this stable semantic memory.

### **1.3 Schema Theory and the Cognitive Scaffolding of Learning**

The utility of KGs in education is further grounded in **schema theory**, a foundational concept in cognitive science. A "schema" is a cognitive structure that serves as a framework for organizing and interpreting information.9 Piaget and Bartlett posited that learning is not the passive accumulation of facts, but the active process of assimilating new information into existing schemas or accommodating schemas to fit new information.

When a student struggles with a complex topic like "Quantum Mechanics," it is often because they lack the requisite schema to organize the incoming data. The isolated facts (e.g., "wave function," "superposition") have no "hooks" to attach to in long-term memory.11 An EduKG acts as an **externalized schema**. By explicitly visualizing the relationships between concepts (e.g., Wave Function \--*determines*\--\> Probability Density), the graph provides the structural scaffolding that the novice learner lacks.

Neuroscientific evidence suggests that the brain utilizes distinct systems for "map-like" (Euclidean) and "graph-like" (topological) navigation.13 The **Cognitive Graph** theory proposes that humans navigate conceptual spaces using a mechanism analogous to spatial navigation—moving from node to node via relational edges. Therefore, presenting information as a graph leverages the brain's innate navigational hardware. This "Cognitive Scaffolding" effect is particularly powerful when dealing with interdisciplinary content, as the graph can bridge schemas from different domains (e.g., linking "Graph Theory" in Math to "Network Analysis" in Sociology).14

Furthermore, expert-novice differences in learning are largely characterized by the quality of these schemas. Experts possess deep, interconnected schemas that allow them to see patterns and underlying principles, whereas novices see disparate, unconnected facts. The automated construction of an EduKG from an expert's lecture is, effectively, an attempt to capture the expert's schema and make it visible to the novice, thereby accelerating the transition from novice to expert understanding.12

---

## **2\. The Ingestion Layer: Signal Processing and Style Transfer**

The journey from a raw lecture video to a pristine knowledge graph begins with the ingestion of the pedagogical signal. This phase is fraught with challenges, as the raw input—spontaneous spoken language—is fundamentally different from the structured, formal language of textbooks upon which most extraction models are trained.

### **2.1 The ASR-NLP Gap and the "Recording for Eyes" Methodology**

While Automatic Speech Recognition (ASR) systems like OpenAI's Whisper have achieved impressive low Word Error Rates (WER) on standard benchmarks 16, a significant **ASR-NLP gap** remains. Spontaneous speech is replete with disfluencies (um, uh), false starts, self-corrections, and grammatical fragmentation. These artifacts, while natural to the ear, are catastrophic for downstream Natural Language Processing (NLP) tasks like Named Entity Recognition (NER) and Relation Extraction (RE).18 A sentence like "The... uh... the Eigen-value, distinct from the vector, is... essentially it's a scalar" contains the core proposition (Eigenvalue, is, scalar), but the syntactic noise makes it difficult for standard parsers to extract it accurately.

To bridge this gap, we must adopt the **Contextualized Spoken-to-Written (CoS2W)** conversion methodology, a process poetically termed "Recording for Eyes, Not Echoing to Ears".20 This task is distinct from simple transcription; it is a form of **modality translation**.

#### **2.1.1 The CoS2W Architecture**

The CoS2W framework utilizes Large Language Models (LLMs) with in-context learning to rewrite ASR transcripts. The architecture operates on a document level, rather than a sentence level, to capture long-range dependencies and context.21 The process involves several transformations:

1. **Disfluency Removal:** Stripping filler words and hesitation markers.  
2. **Grammatical Repair:** converting fragmented utterances into complete, syntactically valid sentences.  
3. **Inverse Text Normalization:** Converting spoken numbers and symbols into their written forms (e.g., "five percent" to "5%").  
4. **Register Elevation:** Transforming the informal, colloquial tone of speech into the formal register of academic prose.22

Recent benchmarks using the **SWAB (Spoken-to-Written conversion of ASR Transcripts Benchmark)** dataset demonstrate that LLMs are highly effective at this task. Evaluation metrics show that LLMs acting as evaluators have high Spearman correlations with human judgments regarding the "faithfulness" (did we keep the meaning?) and "formality" (does it read like a text?) of the conversion.22 By preprocessing lectures through a CoS2W pipeline, we effectively "normalize" the input data, making the lecture transcript semantically indistinguishable from a textbook or a paper, thus enabling high-precision extraction in subsequent steps.

### **2.2 Handling the "Long-Tail" Entity Problem in Science**

A pervasive issue in scientific transcription is the **long-tail entity problem**. Academic lectures often focus on the cutting edge of research, discussing rare entities—specific proteins, obscure theorems, newly named algorithms, or niche historical figures—that appear infrequently in the general training corpora of ASR and LLM models.24

Standard ASR systems, optimized for common language, often hallucinate phonetic approximations for these rare terms (e.g., transcribing "t-SNE" as "tea sneeze"). This creates "noisy nodes" in the knowledge graph that degrade its utility. To mitigate this, we employ **global similarity** techniques for few-shot entity prediction.26

#### **2.2.1 Global Similarity and Weighted Sampling**

Traditional methods for handling rare entities rely on local neighborhood aggregation (e.g., GNNs looking at direct neighbors). However, for a newly introduced entity in a lecture, there are no neighbors yet. The "Global Similarity" approach addresses this by computing the similarity of the rare entity to *all* other entities in the graph, leveraging the global graph structure to infer the entity's type and role.26

Furthermore, an **incremental training framework** with a **weighted sampling strategy** is crucial. This strategy assigns higher importance to edges involving infrequent entities during the training or extraction phase.27 By up-weighting the "long tail," the model becomes more sensitive to rare terms, reducing the likelihood that they are discarded as noise. This is particularly important for **Temporal Knowledge Graphs (TKGs)**, where new entities appear over time and the graph must evolve without "catastrophic forgetting" of previous knowledge.28

### **2.3 Multimodal Representation Learning: Bridging Text and Visuals**

Lectures are rarely purely auditory; they are accompanied by slides, diagrams, and equations. A comprehensive EduKG must be **multimodal**. The challenge lies in the **modality gap**—the misalignment between the vector space of the text and the vector space of the visual content.31

To compress a lecture slide effectively, we must link the visual representation of a "Neural Network Architecture" to its textual description. Recent advances in **Multimodal Representation Learning** utilize contrastive training objectives with a temperature parameter ($\\tau$). Increasing $\\tau$ softens the penalty for hard negatives, which has been shown to reduce the modality gap and create a more semantically coherent latent space.31 This allows the system to cluster textual nodes (e.g., the concept "convolution") with their corresponding visual nodes (e.g., the image of a sliding filter), creating a unified "multimodal node" in the EduKG. This is essential for subjects like geometry or histology, where the "definition" of a concept is inherently visual.32

---

## **3\. Ontological Foundations: The Schema of Science**

Once the raw signal is processed into clean text, the system must interpret the data. This requires an ontology—a controlled vocabulary that defines the types of entities that exist and the relationships that can hold between them. Without an ontology, a knowledge graph is merely a "soup" of strings; with an ontology, it becomes a structured database of meaning.

### **3.1 The SPAR Ontologies: structuring the Scholarly Record**

For academic papers and technical lectures, generic ontologies (like Schema.org) are insufficient. We require the precision of the **Semantic Publishing and Referencing (SPAR)** ontologies.33 The SPAR suite is a collection of orthogonal ontology modules that cover every aspect of the scholarly domain:

* **FaBiO (FRBR-aligned Bibliographic Ontology):** This ontology handles the description of entities. It distinguishes between the "Work" (the conceptual content), the "Expression" (the specific version, e.g., preprint vs. published), and the "Manifestation" (PDF vs. HTML). In an EduKG, a lecture video is a *Manifestation* of a *Work* (the lesson topic).33  
* **CiTO (Citation Typing Ontology):** Standard citation graphs only indicate *that* Paper A cites Paper B. CiTO allows us to characterize the *nature* of that citation. Is it cito:extends? cito:refutes? cito:usesMethodFrom? This semantic richness is vital for students tracing the genealogy of an idea.33  
* **BiRO (Bibliographic Reference Ontology):** Handles the structure of reference lists and bibliographies.  
* **DoCO (Document Components Ontology):** Describes the structural components of a document (e.g., Abstract, Introduction, Methods). This is useful for segmenting lecture transcripts into logical "chapters".35

By mapping extracted entities to these SPAR classes, we ensure that the EduKG is interoperable with the broader semantic web of science.

### **3.2 The Computer Science Ontology (CSO)**

For the specific domain of computer science education, the **Computer Science Ontology (CSO)** is the gold standard. Generated automatically from a massive corpus of scientific articles, CSO contains over 14,000 research topics and 160,000 relationships.37

Integrating CSO into the pipeline involves using the **CSO Classifier**, a Python-based tool that scans the lecture transcript and annotates it with CSO topics.37

* **Hierarchical Inference:** If the classifier detects "LSTM" and "GRU" in the text, it can map them to the super-topic "Recurrent Neural Networks," even if that term is never explicitly mentioned.  
* **Topic Disambiguation:** It distinguishes between polysemous terms (e.g., "Process" in Operating Systems vs. "Process" in Business Management) based on the branch of the ontology.

This external knowledge injection is a form of **semantic expansion**, enriching the graph with hierarchical relationships (superTopicOf, contributesTo) that are implicit in the domain knowledge but explicit in the ontology.38

### **3.3 Automated Ontology Extension via LLMs**

The rate of scientific progress means that any static ontology is perpetually out of date. "Prompt Engineering," for instance, was not a major category in ontologies from 2019\. Therefore, the EduKG pipeline must feature **Automated Ontology Extension**.

Recent methodologies employ LLMs as "ontology engineers".41 When the system encounters a cluster of terms in a lecture that do not fit the existing schema (e.g., "Chain-of-Thought," "Tree-of-Thoughts"), it prompts an LLM to:

1. **Propose a new Class:** Define the new concept.  
2. **Suggest Placement:** Determine where it fits in the hierarchy (e.g., Prompting Strategy $\\subseteq$ In-Context Learning).  
3. **Validate:** Use **Competency Questions (CQs)** to test the validity of the new structure (e.g., "Does this new class allow us to answer queries we couldn't answer before?").43  
4. **Formalize:** Convert the proposal into OWL/RDF format for integration.44

To prevent "ontology drift" or the introduction of circular logic, this process is often mediated by a "Human-in-the-Loop" or rigorous logical validators (e.g., checking for consistency with OWL reasoners).43

---

## **4\. Entity Extraction and Resolution: From Text to Triples**

With the ontology defined, the system moves to the extraction phase. This involves identifying specific instances of classes (Entities) and the connections between them (Relations).

### **4.1 Ontology-Guided Named Entity Recognition (NER)**

Traditional NER models are often trained on news corpora (identifying Person, Location, Organization). For academic content, this is insufficient. We need to identify Method, Metric, Task, Dataset, and Theorem.

We employ **Ontology-Guided NER**, where the extraction is constrained by the target ontology (e.g., CSO or UMLS).46 This can be achieved via:

* **Contrastive Learning:** Fine-tuning embedding models (like BERT or RoBERTa) using contrastive loss to pull the embeddings of entity mentions closer to the embeddings of their ontology definitions.47  
* **Prompt-Based Extraction:** Using LLMs with specific prompts that include the ontology constraints. For example: "Extract all entities that are subclasses of cs:MachineLearningModel from the text."

### **4.2 Coreference Resolution with Deterministic Sieves**

A major challenge in lecture transcripts is the prevalence of pronouns. A lecturer might say, "Transformer models are powerful. *They* use self-attention. *This mechanism* allows for parallelization." To build a graph, we must resolve "*They*" and "*This mechanism*" back to "Transformer models."

While end-to-end neural models exist, **deterministic sieve architectures** offer a compelling balance of precision and transparency for this task.49 This approach applies a sequence of "sieves" (rules) from highest precision to lowest:

1. **Exact String Match:** Linking "Transformers" to "Transformers."  
2. **Strict Head Match:** Linking "The Transformer model" to "Transformers."  
3. **Pronoun Resolution:** Using syntactic parse trees to resolve "it" or "they" to the nearest compatible noun phrase.

Integrating coreference resolution *before* relation extraction is critical. It increases the recall of rare relations by over 20% by ensuring that facts stated about "it" are correctly attributed to the main entity.50

### **4.3 Entity Linking to Global Graphs: OpenAlex and S2AG**

An isolated EduKG is of limited value. To support "discovery," internal nodes must be linked to the global web of science. We utilize APIs to link extracted entities to **OpenAlex** and **Semantic Scholar (S2AG)**.51

* **OpenAlex Integration:** Using the pyalex library, we can query the OpenAlex database to find the canonical ID for a paper or concept mentioned in class.52 This enriches the local node with metadata: the author's institution, the publication year, and the citation count.  
* **Semantic Scholar Integration:** S2AG provides "semantic features" such as identifying ​*influential citations* (citations that significantly impacted the paper) and providing "TLDR" (Too Long; Didn't Read) generative summaries.54

**Table 1: Comparison of Global Knowledge Sources for Entity Linking**

| Feature | OpenAlex | Semantic Scholar (S2AG) | Computer Science Ontology (CSO) |
| :---- | :---- | :---- | :---- |
| **Primary Unit** | Works, Authors, Institutions | Papers, Semantic Features | Research Topics |
| **Access Method** | Free REST API, pyalex | API, Dataset Snapshots | OWL/RDF Download, CSO Classifier |
| **Best For...** | Bibliographic metadata, citation networks | Summarization (TLDRs), citation intent | Topic classification, hierarchy |
| **Entity Scale** | \~250M Works | \~200M Papers | \~14K Topics |
| **Update Frequency** | Monthly Snapshots | Continuous | Periodic Versions (e.g., CSO 3.2) |

---

## **5\. GraphRAG: The Architecture of Semantic Retrieval**

Once the graph is constructed, how do we query it? Standard Retrieval-Augmented Generation (RAG) uses vector similarity to find relevant text chunks. However, vector RAG fails at **global sensemaking**—answering questions that require synthesizing information across the entire corpus (e.g., "How has the concept of 'Attention' evolved throughout this course?").57 To address this, we adopt **GraphRAG**.

### **5.1 GraphRAG vs. Vector RAG**

Vector RAG is analogous to a keyword search on steroids; it finds specific *pieces* of information. GraphRAG is analogous to traversing a mind map; it finds *patterns* and *structures* of information.

**Table 2: Standard RAG vs. GraphRAG in Educational Contexts**

| Feature | Standard Vector RAG | GraphRAG | Implication for Education |
| :---- | :---- | :---- | :---- |
| **Data Structure** | Flat list of text chunks | Knowledge Graph \+ Community Hierarchies | GraphRAG mimics the "Prerequisite" structure of learning. |
| **Retrieval Mechanism** | Cosine similarity (Vector Space) | Graph Traversal \+ Community Summaries | Vector RAG is better for specific definitions; GraphRAG for thematic synthesis. |
| **Summarization Scope** | Local (Chunk-level) | Global (Corpus-level) | GraphRAG can answer "What are the main themes of this course?" |
| **Entity Handling** | Implicit (in embedding) | Explicit (Nodes/Edges) | GraphRAG explicitly links rare scientific terms (Long-tail).26 |
| **Performance (Math QA)** | **Higher F1** for page retrieval 62 | Lower F1 (Retrieves excessive context) | Hybrid approach is required for textbooks. |

### **5.2 Hierarchical Community Detection: The Engine of Global Search**

The core innovation of Microsoft's GraphRAG implementation is the use of **hierarchical community detection** to pre-compute summaries.59

1. **Community Detection:** Algorithms like **Leiden** or **Louvain** are applied to the graph.61 These algorithms partition the graph into modular clusters (communities) based on edge density. The Leiden algorithm is preferred over Louvain as it guarantees well-connected communities and avoids the "disconnected communities" problem.64  
2. **Hierarchy Generation:** This is done recursively.  
   * *Level 0:* The entire graph.  
   * *Level 1:* Broad clusters (e.g., "Machine Learning," "Databases").  
   * *Level 2:* Sub-clusters (e.g., "Deep Learning," "Reinforcement Learning").  
   * *Level 3:* Fine-grained topics (e.g., "CNNs," "RNNs").  
3. **Community Summarization:** An LLM generates a textual summary for *each* community at *each* level.65

### **5.3 Query Modes: Global, Local, and DRIFT**

GraphRAG enables distinct query modes that vector RAG cannot support 60:

* **Global Search:** Used for high-level summarization ("What are the key takeaways of this course?"). The system retrieves the pre-computed summaries from Level 1 or Level 2 communities and synthesizes them. This is a "Map-Reduce" operation over the graph structure.67  
* **Local Search:** Used for specific questions about entities ("What did the lecturer say about 'Gradient Descent'?"). The system identifies the "Gradient Descent" node, retrieves its direct neighbors (1-hop or 2-hop), and uses that subgraph as context.  
* **DRIFT Search (Dynamic Retrieval and Information Flow Traversal):** A hybrid mode that starts with Local Search but "drifts" to related communities if the local context is insufficient. This helps in multi-hop reasoning where the answer lies in the connection between two seemingly disparate topics.60

### **5.4 Critique: The "Excessive Context" Problem**

While GraphRAG excels at synthesis, it is not a panacea. Recent comparative studies on **Page-Level Retrieval** in math textbooks show that GraphRAG can underperform Vector RAG on F1 scores for specific fact retrieval.62 The entity-based structure tends to pull in "excessive" context—related but irrelevant concepts—which can confuse the LLM generator. Thus, a robust educational pipeline should be **hybrid**: using Vector RAG for "Lookup" tasks and GraphRAG for "Synthesis" tasks.69

---

## **6\. The Pipeline Implementation: A Pythonic Approach**

Translating these theories into a functional software pipeline requires orchestrating multiple libraries. We define a "Video2KG" pipeline based on current best practices.71

### **6.1 Step-by-Step Architecture**

1. **Signal Acquisition & Preprocessing:**  
   * **Tool:** ffmpeg for audio extraction; OpenAI Whisper for transcription.  
   * **Refinement:** Pass the raw transcript through a **CoS2W** module (using GPT-4 or Llama-3) to normalize the text (remove timestamps, fix disfluencies).20  
2. **Entity & Relation Extraction (The "LLM-as-Extractor"):**  
   * **Tool:** LangChain or Instructor (for structured Pydantic outputs).  
   * **Method:** Feed chunks of clean text to an LLM with a system prompt enforcing the schema (e.g., "Extract entities mapping to the CSO ontology").  
   * **Structure:** Output must be a list of Triples: (Subject, Predicate, Object).  
   * **Example:** "Marie Curie discovered Radium" $\\rightarrow$ (Marie Curie, discovered, Radium).74  
3. **Graph Construction & Enrichment:**  
   * **Tool:** NetworkX for in-memory graph manipulation.75  
   * **Enrichment:** Use pyalex to query OpenAlex and add attributes to nodes (e.g., add "Citation Count" to a "Paper" node).52  
   * **Community Detection:** Run algorithms.community.leiden to assign community IDs to nodes.  
4. **Storage & Indexing:**  
   * **Tool:** Neo4j as the Graph Database. Use the Neo4jVector store to index node embeddings for hybrid search.76  
   * **Indexing:** Microsoft GraphRAG indexing engine creates the community summaries at this stage.60  
5. **Visualization & Interface:**  
   * **Tool:** pyvis or FalkorDB for rendering interactive HTML graphs.78  
   * **UX:** Allow learners to toggle between "Graph View" (structural) and "Chat View" (RAG-based Q\&A).

### **6.2 Python Implementation Snippet**

A simplified extraction logic using Instructor for strict schema adherence:

Python

import instructor  
from openai import OpenAI  
from pydantic import BaseModel, Field  
from typing import List

\# 1\. Define the Schema (Ontological Constraints)  
class Node(BaseModel):  
    id: str  
    label: str  
    type: str \= Field(description="Type of entity, e.g., Concept, Person, Theorem")

class Edge(BaseModel):  
    source: str  
    target: str  
    relation: str \= Field(description="Predicate, e.g., 'defines', 'depends\_on'")

class KnowledgeGraph(BaseModel):  
    nodes: List\[Node\]  
    edges: List\[Edge\]

\# 2\. Initialize Client with Instructor  
client \= instructor.from\_openai(OpenAI())

def extract\_graph(text\_chunk: str) \-\> KnowledgeGraph:  
    """  
    Extracts a semantic graph from a lecture segment using CoS2W-cleaned text.  
    """  
    return client.chat.completions.create(  
        model="gpt-4o",  
        response\_model=KnowledgeGraph,  
        messages=,  
    )

\# 3\. Usage within Pipeline  
\# assume 'cleaned\_transcript' is the output of the CoS2W module  
kg\_data \= extract\_graph(cleaned\_transcript)  
\#... (Insert into Neo4j or NetworkX)

---

## **7\. Algorithmic Enhancements for Robustness**

To move beyond a toy model, we must address the specific failure modes of KGs in scientific domains.

### **7.1 Global Similarity for Long-Tail Entity Prediction**

As noted, rare entities often lack local connectivity. We implement a **Model-Agnostic Enhancement Layer** that computes entity similarity using global graph information.26

* **Mechanism:** Instead of learning embeddings solely from neighbors ($N(v)$), we learn from the set of all entities ($V$) weighted by their structural role similarity.  
* **Weighted Sampling:** During training, we over-sample edges connected to rare entities to prevent the model from overfitting to common concepts (e.g., "Deep Learning") and ignoring rare ones (e.g., "Gumbel-Softmax").27

### **7.2 Temporal Dynamics**

Educational content is linear and temporal. Concept A is introduced at minute 5; Concept B at minute 10\. A standard KG flattens this time dimension. A **Temporal Knowledge Graph (TKG)** preserves it by adding a time attribute to edges: (Concept A, precedes, Concept B, t=5min). This allows for "prerequisite checking"—the system can warn a student if they are skipping to a concept without having visited its temporal predecessors.26

---

## **8\. Conclusion and Future Outlook**

The construction of Knowledge Graphs from lecture content is a sophisticated exercise in **semantic compression**. It leverages the **lattice theory of information** to mathematically justify the abstraction of raw speech into structured propositions, and **schema theory** to justify the pedagogical utility of these structures for human learning.

By integrating advanced signal processing (**CoS2W**) to handle the messy reality of spoken language, rigorous **ontologies** (SPAR, CSO) to align with global science, and **GraphRAG** architectures to enable hierarchical reasoning, we can transform the passive archive of lecture videos into an active, navigable, and intelligent tutor.

The future of this technology lies in **autonomy** and **interoperability**. Autonomous agents will likely crawl university repositories, automatically generating and updating these graphs, while interoperable standards will allow a student's "Personal Knowledge Graph" to travel with them from course to course, growing and evolving as they traverse the lattice of human knowledge. The ultimate goal is to reduce the friction of learning, allowing the human mind to focus not on the retention of facts, but on the mastery of concepts.

**Table 3: The EduKG Technology Stack**

| Component | Recommended Tool/Library | Function | Source |
| :---- | :---- | :---- | :---- |
| **ASR** | OpenAI Whisper | Transcribe video to raw text | 16 |
| **Text Cleaning** | GPT-4 / Llama 3 | CoS2W (Spoken-to-Written) conversion | 20 |
| **Graph Extraction** | LangChain \+ Instructor | Extract structured triples from text | 73 |
| **Graph Storage** | Neo4j | Graph Database & Cypher Querying | 76 |
| **Graph Analysis** | NetworkX | Centrality, pathfinding, clustering | 75 |
| **Entity Linking** | pyalex (OpenAlex) | Link nodes to scientific literature | 52 |
| **Ontology** | CSO Classifier | Classify topics using CS Ontology | 37 |
| **RAG Architecture** | Microsoft GraphRAG | Global search & community summaries | 60 |
| **Visualization** | pyvis / FalkorDB | Interactive graph exploration | 78 |

#### **Works cited**

1. Semantic Compression with Information Lattice Learning \- OpenReview, accessed November 19, 2025, [https://openreview.net/pdf?id=Me4WnG7YXc](https://openreview.net/pdf?id=Me4WnG7YXc)  
2. Semantic compression \- Wikipedia, accessed November 19, 2025, [https://en.wikipedia.org/wiki/Semantic\_compression](https://en.wikipedia.org/wiki/Semantic_compression)  
3. Statistical Mechanics of Semantic Compression \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2503.00612v1](https://arxiv.org/html/2503.00612v1)  
4. Semantic Compression with Information Lattice Learning \- IEEE Xplore, accessed November 19, 2025, [https://ieeexplore.ieee.org/document/10591764/](https://ieeexplore.ieee.org/document/10591764/)  
5. Semantic Compression with Information Lattice Learning \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2404.03131v1](https://arxiv.org/html/2404.03131v1)  
6. Semantic Communication: A Survey of Its Theoretical Development \- MDPI, accessed November 19, 2025, [https://www.mdpi.com/1099-4300/26/2/102](https://www.mdpi.com/1099-4300/26/2/102)  
7. Semantic Maps for Knowledge Graphs: A Semantic-Based Summarization Approach \- IEEE Xplore, accessed November 19, 2025, [https://ieeexplore.ieee.org/iel7/6287639/10380310/10384364.pdf](https://ieeexplore.ieee.org/iel7/6287639/10380310/10384364.pdf)  
8. Enhancing Text-based Knowledge Graph Completion with Zero-Shot Large Language Models \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2310.08279v3](https://arxiv.org/html/2310.08279v3)  
9. Schema Theory In Psychology, accessed November 19, 2025, [https://www.simplypsychology.org/what-is-a-schema.html](https://www.simplypsychology.org/what-is-a-schema.html)  
10. A Complete Guide to Schema Theory and its Role in Education, accessed November 19, 2025, [https://www.educationcorner.com/schema-theory/](https://www.educationcorner.com/schema-theory/)  
11. EEF blog: Working with schemas and why it matters to teachers, accessed November 19, 2025, [https://educationendowmentfoundation.org.uk/news/eef-blog-working-with-schemas-and-why-it-matters-to-teachers](https://educationendowmentfoundation.org.uk/news/eef-blog-working-with-schemas-and-why-it-matters-to-teachers)  
12. What Is Schema? How Do We Help Students Build It? (Opinion) \- Education Week, accessed November 19, 2025, [https://www.edweek.org/teaching-learning/opinion-what-is-schema-how-do-we-help-students-build-it/2019/10](https://www.edweek.org/teaching-learning/opinion-what-is-schema-how-do-we-help-students-build-it/2019/10)  
13. Structuring Knowledge with Cognitive Maps and Cognitive Graphs \- PMC \- PubMed Central, accessed November 19, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC7746605/](https://pmc.ncbi.nlm.nih.gov/articles/PMC7746605/)  
14. Graph RAG Use Cases: Real-World Applications & Examples \- Chitika, accessed November 19, 2025, [https://www.chitika.com/uses-of-graph-rag/](https://www.chitika.com/uses-of-graph-rag/)  
15. 356 Building a Learning Path Recommender \- Manual Construction of Knowledge Graphs in Python \- YouTube, accessed November 19, 2025, [https://www.youtube.com/watch?v=LSuNRtw5f3A](https://www.youtube.com/watch?v=LSuNRtw5f3A)  
16. Improving Named Entity Transcription with Contextual LLM-based Revision \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2506.10779v1](https://arxiv.org/html/2506.10779v1)  
17. Measuring the Accuracy of Automatic Speech Recognition Solutions \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2408.16287v1](https://arxiv.org/html/2408.16287v1)  
18. Comparing Named Entity Recognition on Transcriptions and Written Texts \- ResearchGate, accessed November 19, 2025, [https://www.researchgate.net/publication/283101103\_Comparing\_Named\_Entity\_Recognition\_on\_Transcriptions\_and\_Written\_Texts](https://www.researchgate.net/publication/283101103_Comparing_Named_Entity_Recognition_on_Transcriptions_and_Written_Texts)  
19. Why Aren't We NER Yet? Artifacts of ASR Errors in Named Entity Recognition in Spontaneous Speech Transcripts \- ACL Anthology, accessed November 19, 2025, [https://aclanthology.org/2023.acl-long.98.pdf](https://aclanthology.org/2023.acl-long.98.pdf)  
20. Recording for Eyes, Not Echoing to Ears: Contextualized Spoken-to-Written Conversion of ASR Transcripts \- AAAI Publications, accessed November 19, 2025, [https://ojs.aaai.org/index.php/AAAI/article/view/34642/36797](https://ojs.aaai.org/index.php/AAAI/article/view/34642/36797)  
21. Recording for Eyes, Not Echoing to Ears: Contextualized Spoken-to-Written Conversion of ASR Transcripts \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2408.09688v2](https://arxiv.org/html/2408.09688v2)  
22. Recording for Eyes, Not Echoing to Ears: Contextualized Spoken-to-Written Conversion of ASR Transcripts | Request PDF \- ResearchGate, accessed November 19, 2025, [https://www.researchgate.net/publication/390700730\_Recording\_for\_Eyes\_Not\_Echoing\_to\_Ears\_Contextualized\_Spoken-to-Written\_Conversion\_of\_ASR\_Transcripts](https://www.researchgate.net/publication/390700730_Recording_for_Eyes_Not_Echoing_to_Ears_Contextualized_Spoken-to-Written_Conversion_of_ASR_Transcripts)  
23. Recording for Eyes, Not Echoing to Ears: Contextualized Spoken-to-Written Conversion of ASR Transcripts \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2408.09688v4](https://arxiv.org/html/2408.09688v4)  
24. Tackling Long-Tailed Relations and Uncommon Entities in Knowledge Graph Completion, accessed November 19, 2025, [https://aclanthology.org/D19-1024/](https://aclanthology.org/D19-1024/)  
25. Knowledge Graphs: Opportunities and Challenges \- PMC \- PubMed Central, accessed November 19, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC10068207/](https://pmc.ncbi.nlm.nih.gov/articles/PMC10068207/)  
26. Tackling Long-Tail Entities for Temporal Knowledge Graph Completion | Request PDF, accessed November 19, 2025, [https://www.researchgate.net/publication/380539759\_Tackling\_Long-Tail\_Entities\_for\_Temporal\_Knowledge\_Graph\_Completion](https://www.researchgate.net/publication/380539759_Tackling_Long-Tail_Entities_for_Temporal_Knowledge_Graph_Completion)  
27. Towards Improving Long-Tail Entity Predictions in Temporal Knowledge Graphs through Global Similarity and Weighted Sampling \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2507.18977v1](https://arxiv.org/html/2507.18977v1)  
28. \[2507.18977\] Towards Improving Long-Tail Entity Predictions in Temporal Knowledge Graphs through Global Similarity and Weighted Sampling \- arXiv, accessed November 19, 2025, [https://arxiv.org/abs/2507.18977](https://arxiv.org/abs/2507.18977)  
29. Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution Networks \- ACL Anthology, accessed November 19, 2025, [https://aclanthology.org/N19-1306.pdf](https://aclanthology.org/N19-1306.pdf)  
30. Towards Improving Long-Tail Entity Predictions in Temporal Knowledge Graphs through Global Similarity and Weighted Sampling \- ChatPaper, accessed November 19, 2025, [https://chatpaper.com/paper/170486](https://chatpaper.com/paper/170486)  
31. Semantic Compression via Multimodal Representation Learning \- ResearchGate, accessed November 19, 2025, [https://www.researchgate.net/publication/395968890\_Semantic\_Compression\_via\_Multimodal\_Representation\_Learning](https://www.researchgate.net/publication/395968890_Semantic_Compression_via_Multimodal_Representation_Learning)  
32. Light Up the Shadows: Enhance Long-Tailed Entity Grounding with Concept-Guided Vision-Language Models \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2406.10902v1](https://arxiv.org/html/2406.10902v1)  
33. Semantic Publishing and Referencing Ontologies, accessed November 19, 2025, [http://www.sparontologies.net/ontologies](http://www.sparontologies.net/ontologies)  
34. The SPAR Ontologies, accessed November 19, 2025, [https://sparontologies.github.io/article/spar-iswc2018/](https://sparontologies.github.io/article/spar-iswc2018/)  
35. Publications \- SPAR Ontologies, accessed November 19, 2025, [http://www.sparontologies.net/publications](http://www.sparontologies.net/publications)  
36. Introducing the Semantic Publishing and Referencing (SPAR) Ontologies, accessed November 19, 2025, [https://opencitations.wordpress.com/2010/10/14/introducing-the-semantic-publishing-and-referencing-spar-ontologies/](https://opencitations.wordpress.com/2010/10/14/introducing-the-semantic-publishing-and-referencing-spar-ontologies/)  
37. Computer Science Ontology \- Wikipedia, accessed November 19, 2025, [https://en.wikipedia.org/wiki/Computer\_Science\_Ontology](https://en.wikipedia.org/wiki/Computer_Science_Ontology)  
38. Computer Science Ontology (CSO) \- OntoLearner 0.1.0 documentation, accessed November 19, 2025, [https://ontolearner.readthedocs.io/benchmarking/scholarly\_knowledge/cso.html](https://ontolearner.readthedocs.io/benchmarking/scholarly_knowledge/cso.html)  
39. The Computer Science Ontology: A Comprehensive Automatically-Generated Taxonomy of Research Areas | Data Intelligence \- MIT Press Direct, accessed November 19, 2025, [https://direct.mit.edu/dint/article/2/3/379/94891/The-Computer-Science-Ontology-A-Comprehensive](https://direct.mit.edu/dint/article/2/3/379/94891/The-Computer-Science-Ontology-A-Comprehensive)  
40. CSO \- Portal \- Computer Science Ontology, accessed November 19, 2025, [https://cso.kmi.open.ac.uk/about](https://cso.kmi.open.ac.uk/about)  
41. Large Language Models for Ontology Engineering: A Systematic Literature Review \- Semantic Web Journal, accessed November 19, 2025, [https://www.semantic-web-journal.net/system/files/swj3864.pdf](https://www.semantic-web-journal.net/system/files/swj3864.pdf)  
42. Ontology Engineering with Large Language Models: Unveiling the potential of human-LLM collaboration in the ontology extension process \- Maastricht University, accessed November 19, 2025, [https://cris.maastrichtuniversity.nl/en/publications/ontology-engineering-with-large-language-models-unveiling-the-pot/](https://cris.maastrichtuniversity.nl/en/publications/ontology-engineering-with-large-language-models-unveiling-the-pot/)  
43. Methodological Exploration of Ontology Generation with a Dedicated Large Language Model \- MDPI, accessed November 19, 2025, [https://www.mdpi.com/2079-9292/14/14/2863](https://www.mdpi.com/2079-9292/14/14/2863)  
44. Ontology Generation using Large Language Models \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2503.05388v1](https://arxiv.org/html/2503.05388v1)  
45. Ontology extension by online clustering with large language model agents \- PMC \- NIH, accessed November 19, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC11491333/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11491333/)  
46. Ontology-Guided Query Expansion for Biomedical Document Retrieval using Large Language Models \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2508.11784v1](https://arxiv.org/html/2508.11784v1)  
47. Relation Extraction in Biomedical Abstracts with LLMs and Ontology Constraints \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2509.19057v1](https://arxiv.org/html/2509.19057v1)  
48. Towards Ontology-Enhanced Representation Learning for Large Language Models \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2405.20527v1](https://arxiv.org/html/2405.20527v1)  
49. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules, accessed November 19, 2025, [https://direct.mit.edu/coli/article/39/4/885/1463/Deterministic-Coreference-Resolution-Based-on](https://direct.mit.edu/coli/article/39/4/885/1463/Deterministic-Coreference-Resolution-Based-on)  
50. \[2509.17289\] Automated Knowledge Graph Construction using Large Language Models and Sentence Complexity Modelling \- arXiv, accessed November 19, 2025, [https://arxiv.org/abs/2509.17289](https://arxiv.org/abs/2509.17289)  
51. The Semantic Scholar Open Data Platform \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2301.10140v2](https://arxiv.org/html/2301.10140v2)  
52. pyalex \- PyPI, accessed November 19, 2025, [https://pypi.org/project/pyalex/](https://pypi.org/project/pyalex/)  
53. openalex-analysis \- PyPI, accessed November 19, 2025, [https://pypi.org/project/openalex-analysis/](https://pypi.org/project/openalex-analysis/)  
54. LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows Published in TPDL 2025, New Trends in Theory and Practice of Digital Libraries, Communications in Computer and Information Science, vol 2694\. DOI 10.1007/978-3-032-06136-2\_9. This PDF is the author-prepared camera-ready version corresponding \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2510.04749v1](https://arxiv.org/html/2510.04749v1)  
55. PubMed knowledge graph 2.0: Connecting papers, patents, and clinical trials in biomedical science \- PMC, accessed November 19, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC12174349/](https://pmc.ncbi.nlm.nih.gov/articles/PMC12174349/)  
56. OpenAlex technical documentation: Overview, accessed November 19, 2025, [https://docs.openalex.org/](https://docs.openalex.org/)  
57. GraphRAG Explained: Enhancing RAG with Knowledge Graphs | by Zilliz \- Medium, accessed November 19, 2025, [https://medium.com/@zilliz\_learn/graphrag-explained-enhancing-rag-with-knowledge-graphs-3312065f99e1](https://medium.com/@zilliz_learn/graphrag-explained-enhancing-rag-with-knowledge-graphs-3312065f99e1)  
58. From Local to Global: A Graph RAG Approach to Query-Focused Summarization \- Microsoft, accessed November 19, 2025, [https://www.microsoft.com/en-us/research/publication/from-local-to-global-a-graph-rag-approach-to-query-focused-summarization/](https://www.microsoft.com/en-us/research/publication/from-local-to-global-a-graph-rag-approach-to-query-focused-summarization/)  
59. From Local to Global: A GraphRAG Approach to Query-Focused Summarization \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2404.16130v2](https://arxiv.org/html/2404.16130v2)  
60. Welcome \- GraphRAG, accessed November 19, 2025, [https://microsoft.github.io/graphrag/](https://microsoft.github.io/graphrag/)  
61. Global Community Summary Retriever \- GraphRAG, accessed November 19, 2025, [https://graphrag.com/reference/graphrag/global-community-summary-retriever/](https://graphrag.com/reference/graphrag/global-community-summary-retriever/)  
62. accessed November 19, 2025, [https://arxiv.org/abs/2509.16780\#:\~:text=Our%20findings%20show%20that%20embedding,to%20its%20entity%2Dbased%20structure.](https://arxiv.org/abs/2509.16780#:~:text=Our%20findings%20show%20that%20embedding,to%20its%20entity%2Dbased%20structure.)  
63. From Local to Global: A Graph RAG Approach to Query-Focused Summarization \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2404.16130v1](https://arxiv.org/html/2404.16130v1)  
64. How Would Microsoft GraphRAG Work Alongside Actual Graph Database? \- Memgraph, accessed November 19, 2025, [https://public-assets.memgraph.com/community-calls/microsoft-graphrag-memgraph.pdf](https://public-assets.memgraph.com/community-calls/microsoft-graphrag-memgraph.pdf)  
65. Unravelling Microsoft GraphRAG's Advanced Retrieval Technique: A Deep Dive on Indexing and Local Search | by Tanmay Odapally | Medium, accessed November 19, 2025, [https://medium.com/@todap/unravelling-microsoft-graphrags-advanced-retrieval-technique-a-deeper-dive-on-indexing-and-local-8c2c41a03f13](https://medium.com/@todap/unravelling-microsoft-graphrags-advanced-retrieval-technique-a-deeper-dive-on-indexing-and-local-8c2c41a03f13)  
66. Overview \- GraphRAG, accessed November 19, 2025, [https://microsoft.github.io/graphrag/query/overview/](https://microsoft.github.io/graphrag/query/overview/)  
67. GraphRAG: Improving global search via dynamic community selection \- Microsoft Research, accessed November 19, 2025, [https://www.microsoft.com/en-us/research/blog/graphrag-improving-global-search-via-dynamic-community-selection/](https://www.microsoft.com/en-us/research/blog/graphrag-improving-global-search-via-dynamic-community-selection/)  
68. \[2509.16780\] Comparing RAG and GraphRAG for Page-Level Retrieval Question Answering on Math Textbook \- arXiv, accessed November 19, 2025, [https://arxiv.org/abs/2509.16780](https://arxiv.org/abs/2509.16780)  
69. Comparing RAG and GraphRAG for Page-Level Retrieval Question Answering on Math Textbook \- ResearchGate, accessed November 19, 2025, [https://www.researchgate.net/publication/395724386\_Comparing\_RAG\_and\_GraphRAG\_for\_Page-Level\_Retrieval\_Question\_Answering\_on\_Math\_Textbook](https://www.researchgate.net/publication/395724386_Comparing_RAG_and_GraphRAG_for_Page-Level_Retrieval_Question_Answering_on_Math_Textbook)  
70. Comparing RAG and GraphRAG for Page-Level Retrieval Question Answering on Math Textbook \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2509.16780v1](https://arxiv.org/html/2509.16780v1)  
71. A Beginner's Guide to Building Knowledge Graphs from Videos ..., accessed November 19, 2025, [https://towardsdatascience.com/a-beginners-guide-to-building-knowledge-graphs-from-videos-6cafcba5f3e5](https://towardsdatascience.com/a-beginners-guide-to-building-knowledge-graphs-from-videos-6cafcba5f3e5)  
72. A Beginner's Guide to Building Knowledge Graphs from Videos | Towards Data Science, accessed November 19, 2025, [https://towardsdatascience.com/a-beginners-guide-to-building-knowledge-graphs-from-videos-6cafcba5f3e5/](https://towardsdatascience.com/a-beginners-guide-to-building-knowledge-graphs-from-videos-6cafcba5f3e5/)  
73. Visualizing Knowledge Graphs: A Guide to Complex Topics \- Instructor, accessed November 19, 2025, [https://python.useinstructor.com/examples/knowledge\_graph/](https://python.useinstructor.com/examples/knowledge_graph/)  
74. FareedKhan-dev/KG-Pipeline: Converting Unstructured Data to a Knowledge Graph, accessed November 19, 2025, [https://github.com/FareedKhan-dev/KG-Pipeline](https://github.com/FareedKhan-dev/KG-Pipeline)  
75. 354 \- Knowledge Graphs in Python Using NetworkX library \- YouTube, accessed November 19, 2025, [https://www.youtube.com/watch?v=n7BTWc2C1Eg](https://www.youtube.com/watch?v=n7BTWc2C1Eg)  
76. Implementing Graph RAG Using Knowledge Graphs \- IBM, accessed November 19, 2025, [https://www.ibm.com/think/tutorials/knowledge-graph-rag](https://www.ibm.com/think/tutorials/knowledge-graph-rag)  
77. User Guide: Knowledge Graph Builder — neo4j-graphrag-python documentation, accessed November 19, 2025, [https://neo4j.com/docs/neo4j-graphrag-python/current/user\_guide\_kg\_builder.html](https://neo4j.com/docs/neo4j-graphrag-python/current/user_guide_kg_builder.html)  
78. How to Build a Knowledge Graph: A Step-by-Step Guide \- FalkorDB, accessed November 19, 2025, [https://www.falkordb.com/blog/how-to-build-a-knowledge-graph/](https://www.falkordb.com/blog/how-to-build-a-knowledge-graph/)  
79. Ep-2. Nodes, Edges and Shard State in LangGraph \- YouTube, accessed November 19, 2025, [https://www.youtube.com/watch?v=Xgyk7Qtmf8s](https://www.youtube.com/watch?v=Xgyk7Qtmf8s)