# 2025-10-15

At long last, the realm education is being completely overhauled and transformed -- one can't help but being completely FASCINATED by the examples of how AI is faciliating student projects in finance and how emergent learning communities are encouraging even more rapid innovation. Consider GauntletAI and Zax Capital and how Zax and GauntletAI students have acquired the assets of bankrupt Marin Software, an adtech company once valued at $425 million that filed for bankruptcy in July 2025; Marin completed its Chapter 11 reorganization on September 5, 2025, where all existing shares were canceled before Zax and the GauntletAI students acquired the assets for what is reported to be $1.4 million in cash and $4.6 million in new notes. Marin's assets include its technology platform, intellectual property, and customer contracts. [In a nutshell](https://g.co/gemini/share/bdcfeb6e0d44), the acquisition is expected to help Zax Capital [a division of ESW Capital], which defines itself as an *AI Transformation HoldCo*, both enhance its portfolio of AI-driven advertising solutions as well as expand its venture capital markets reach. The larger case study of [how GauntletAI and Zax Capital students pulled this off](https://docs.google.com/document/d/120PN774gL1Z1VzwTkDRh7BrCe2ItR3WTPeVQdj3cbAk/edit?usp=sharing) is fascinating and likely to be worth [following along](https://marin.gauntletai.com/) because of how this might actually work and would be a harbinger of things to come ... but mostly in how this approach exemplifies the need to try different things to implement a core philosophy aimed at repudiating the incrementalism and inertia that characterizes both corporate IT and academia. 

Of course, developing the practical business/finance savvy of students who are developers is only one aspect -- our fascination not just with students pulling off AI rollups as class projects in using AI to transform finance.  For a practical nuts-and-bolts focus on primarily just code and collaborative open source projects, can also delve into the [nanochat discussion on GitHub](https://github.com/karpathy/nanochat/discussions/1) as well as the [nanochat student organization at HuggingFace](https://huggingface.co/nanochat-students) for decentralized individual students and autodidacts who are following Andrej Karpathy's [nanochat portion](https://github.com/karpathy/nanochat) of [the LLM 101n course](https://github.com/karpathy/LLM101n) by [Eureka Labs AI](https://eurekalabs.ai/) will be a a Storyteller AI Large Language Model (LLM).  

In 101n LLM course, students are learning to build a full-stack LLM implementation from tokenization to web serving, all for under $100 ... nanochat is similar to, but a much more ambitious expansion of [Karpathy's nanoGPT demonstration repo](https://github.com/karpathy/nanoGPT), from a year or so back. the nanoGPT demo covered only pretraining whereas this LLM 101n course is about *the whole shebang*.

The capstone objective for students is build a Storyteller AI Large Language Model (LLM) ... with Storyteller, anyone will be able to create, refine and illustrate little stories with the AI. The real objective is not the LLM itself, but the practical experience gained by building everything end-to-end from basics to a functioning web app similar to ChatGPT ... all ***from scratch*** in Python, C and CUDA, and with minimal computer science prerequisites. By the end a student would have a relatively deep understanding of AI, LLMs, and deep learning more generally. The syllabus illustrates how the objective will be achieved:

* Chapter 01 Bigram Language Model (language modeling)
* Chapter 02 Micrograd (machine learning, backpropagation)
* Chapter 03 N-gram model (multi-layer perceptron, matmul, gelu)
* Chapter 04 Attention (attention, softmax, positional encoder)
* Chapter 05 Transformer (transformer, residual, layernorm, GPT-2)
* Chapter 06 Tokenization (minBPE, byte pair encoding)
* Chapter 07 Optimization (initialization, optimization, AdamW)
* Chapter 08 Need for Speed I: Device (device, CPU, GPU, ...)
* Chapter 09 Need for Speed II: Precision (mixed precision training, fp16, bf16, fp8, ...)
* Chapter 10 Need for Speed III: Distributed (distributed optimization, DDP, ZeRO)
* Chapter 11 Datasets (datasets, data loading, synthetic data generation)
* Chapter 12 Inference I: kv-cache (kv-cache)
* Chapter 13 Inference II: Quantization (quantization)
* Chapter 14 Finetuning I: SFT (supervised finetuning SFT, PEFT, LoRA, chat)
* Chapter 15 Finetuning II: RL (reinforcement learning, RLHF, PPO, DPO)
* Chapter 16 Deployment (API, web app)
* Chapter 17 Multimodal (VQVAE, diffusion transformer)

Nanochat itself is a minimal, from scratch, full-stack training/inference pipeline of a simple ChatGPT clone in a single, dependency-minimal codebase. Somebody who wants to use it simply boots up a cloud GPU box, run a single script and in as little as 4 hours later you can talk to your own LLM in a ChatGPT-like web UI. Even for as low as ~$100 in cost (~4 hours on an 8XH100 node), you can train a little ChatGPT clone that you can kind of talk to, and which can write stories/poems, answer simple questions. About ~12 hours [or $300 worth of retail compute in 2025] surpasses [GPT-2](https://en.wikipedia.org/wiki/GPT-2) CORE metric, which required $250,000 of wholesale compute resources in 2019 ... so the code itself is probably 2 or even three orders of magnitude more efficient. As you further scale up towards ~$1000 (~41.6 hours of training), nanochat quickly becomes a lot more coherent and can solve simple math/code problems and take multiple choice tests. E.g. a depth 30 model trained for 24 hours (this is about equal to FLOPs of GPT-3 Small 125M and 1/1000th of GPT-3) gets into 40s on MMLU and 70s on ARC-Easy, 20s on GSM8K, etc.

**Karpathy's goal in doing doing this to get the full "strong baseline" stack into one cohesive, maximally forkable repo, strictly as a minimal, readable, hackable demonstration repo just for the purposes of training students with a practical exercise.** 

Karpathy's [nanochat repo](https://github.com/karpathy/nanochat) also has potential to be forked and grow into a research harness, or a benchmark, similar to nanoGPT before it. By no means finished or now irrelevant academic exercise, although it is usable, as a somewhat tuned or optimized body of work. But the extensibility is there, with tons of low-hanging fruit in terms of ways to specifically tailor it to a particular use case ... but it's at a place where the overall skeleton is ok enough that it can go up on GitHub where all the parts of it can be improved.

It weighs ~8,000 lines of relatively clean code ... the basic approach is relatively straightforward and involves:

- Train the tokenizer using a new Rust implementation
- Pretrain a Transformer LLM on FineWeb, evaluate CORE score across a number of metrics
- Midtrain on user-assistant conversations from SmolTalk, multiple choice questions, tool use.
- SFT, evaluate the chat model on world knowledge multiple choice (ARC-E/C, MMLU), math (GSM8K), code (HumanEval)
- RL the model optionally on GSM8K with "GRPO"
- Efficient inference the model in an Engine with KV cache, simple prefill/decode, tool use (Python interpreter in a lightweight sandbox), talk to it over CLI or ChatGPT-like WebUI.
- Write a single markdown report card, summarizing and gamifying the whole thing.