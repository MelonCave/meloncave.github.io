# 2025-10-03

We [***continue***](https://github.com/AncientGuy/PKM/tree/main/journal/2025-09-29.md) to completely RETHINK Personal Knowledge Engineering workflows ... there are several gigantic problems with the thinking on AI ... mostly because completely false assumptions are accepted as gospel ... this is my current understanding of the problem.

* **Most people are even more GLARINGLY ignorant about power consumption than they are about AI -- actual AI power use and how impressive the record over the last few decades has been on monitoring power use as a means to radically improve designs and strategies to drive one or two or more orders of magnitude reduction in compute/watt** ... *and the resultant conversations about AI power use get EVEN more ridiculous than conversations about just AI ... and have EVEN less to do with Reality as the back-and-forth progresses from there ... it gets worse and worse!* So we end up with poisonous IDIOCY and things like advocating the construction of massive power plants OR that compute obviates the need for high quality data or safer, more efficient algorithms ... so, as a result of the ignorance and massive public push toward the shiny features of AI and things like AI-generated blockbuster-movie-quality video ... that there are not enough power plants and nobody believes there ever will enough power plants ... so, in the scramble, we literally have powerplants and transmission lines built or on the drawing board, with even more being built EVERYWHERE humans can see, ie the pollution of scattered power generation and transmission is even worse than how plastic trash replaced cans/bottles as a sign of human civilization. 

**HOWEVER** this kind of idiocy is completely unnecessary -- because people have forgotten than compute will get radically more efficient over the next decade AND algorithms can be radically more efficient, speedy, and safe -- RATHER the kind of lowest-common-denominator, anybody-can-write Javascript/Pythonic crap bs that is now the norm ... *but that takes us into the next sub-topic...*

* **Most people START OFF not understanding linear algebra** ... not just not understanding it, but not even knowing what it is or why problem-solving leads to things like simulataneous equations and the need for algebra and matrix algebra ... *and the resultant conversations about AI get more emotional and have even less to do with Reality as the back-and-forth progresses from there*. The REASON that we ever needed advanced GPUs [in late 80s or early 90s] was that it became **apparent** that CPUs, eg Intel's new 80486 design, did many things extremely well or well enough to bedazzle consumers who had used DOS. The 80486 and competing CPUs ushered in things like GUIs and multi-tasking, multi-window operating systems, but in terms of **serious** scientific compute, CPUs were architecturally multi-tasking jacks-of-all-trades devices, and as such, were just not going to ever be ***focused*** enough on OPTIMIZING the parallelization of the heavy duty workflows with memory and processing power NEEDED for serious matrix manipulation that we have in linear algebra of things like gaming rigs, CAD/simulation workstations, serious machine learning math/stat applications and a variety of scientific computing needs. People who are not familiar with the linear algebra of these kinds of things HAVE ABSOLUTELY NO CLUE about how GPUs or massive levels of compute are implemented ... and thus EVEN LESS than ABSOLUTELY NO CLUE of what AI actually even IS. *This was at first GLARINGLY, and now NAUSEATINGLY APPARENT when one is listening conversations about AI hype or promises or overblown fears of* ***what AI even is.*** 

*Perhaps, it is worth digressing a bit on the history of GPUs, AI and scientific compute ...*

## Graphics processors existed decades before modern GPUs

Graphics display processors emerged in **1982 with the NEC μPD7220**, the first personal computer graphics processor implemented as a single integrated circuit chip. This processor could draw lines, circles, and character graphics at resolutions up to 1024×1024, primarily serving CAD applications and high-resolution professional workstations. Intel licensed this design as the 82720, creating Intel's first graphics processing unit.

Throughout the 1980s, various graphics coprocessors emerged—the Hitachi ARTC HD63484 (1984), Texas Instruments TMS34010 (1986), and IBM 8514 graphics system (1987)—but these were fundamentally different from modern GPUs. The early 1990s brought **2D acceleration** with products like the S3 86C911 (1991), which by 1995 had spawned numerous imitators making 2D acceleration standard.

The 3D revolution began in 1996 when **3dfx released the Voodoo Graphics chipset** in November, achieving 80-85% of the 3D accelerator market. This dedicated 3D-only add-in card transformed PC gaming, requiring pass-through from a separate 2D graphics card. Sony first used the term "GPU" in 1994 for PlayStation's graphics chip, while 3Dlabs called their 1997 Glint Gamma processor a "GPU" (geometry processor unit).

**NVIDIA's GeForce 256, released August 31, 1999**, established the modern GPU concept. Marketed as "the world's first GPU," it integrated hardware Transform & Lighting (T&L) directly into a single-chip processor capable of processing over 10 million polygons per second. This product defined what we now recognize as a GPU—a stark contrast to the simple display processors of the early 1980s.

## The Intel 80486 was a CPU with no graphics capabilities

The Intel 80486 was a **32-bit central processing unit announced in April 1989**, representing the fourth generation of x86 microprocessors. This CPU contained 1.2 million transistors (the first x86 chip exceeding one million), featured an integrated Floating Point Unit, and operated at 20-100 MHz with an 8 KB unified L1 cache.

**The 80486 had zero relationship to GPU development.** Released in 1989—a full decade before the GeForce 256—the 80486 was a general-purpose CPU with no integrated graphics processing capabilities whatsoever. During the 80486 era (1989-1993), graphics were handled by entirely separate VGA cards and early 2D accelerators. Conflating this CPU with GPU history represents a fundamental misunderstanding of computer architecture evolution.

## Matrix manipulation expertise preceded and enabled consumer GPU development

The engineers who built consumer gaming GPUs in the 1990s came from scientific computing and professional workstation companies where matrix mathematics was absolutely central to their work.

**3D graphics rendering inherently requires matrix operations**: transformations, rotations, projections, and lighting calculations are all linear algebra operations. This wasn't discovered accidentally—it was the foundational knowledge of the workstation graphics community throughout the 1980s. Companies like Silicon Graphics (SGI), Evans & Sutherland, and others built high-end systems for CAD, scientific visualization, flight simulation, and movie special effects. The engineers at these companies were deeply versed in linear algebra because you cannot do 3D graphics without it.

The gaming GPU revolution happened when entrepreneurs from this scientific/workstation community recognized that **gaming was an underserved market** that could benefit from the same matrix manipulation hardware, but at consumer price points. Many founders of gaming GPU companies came directly from the workstation graphics industry, bringing their linear algebra expertise with them. They didn't invent matrix manipulation for gaming—they adapted existing scientific computing techniques to make them affordable for consumers.

Gaming created the **mass market and economic scale** that justified dedicated consumer hardware, but the mathematical foundations, algorithmic knowledge, and understanding of why parallel matrix operations matter were established in the scientific computing community first. The workstation industry proved that specialized hardware for matrix-heavy 3D rendering was valuable; gaming made it economically viable at $300 instead of $30,000.

When GPGPU computing emerged in 2001-2003, researchers weren't discovering that GPUs "could" do matrix math—they were recognizing that hardware designed for graphics matrix operations (transforms, projections) could be repurposed for other linear algebra problems. The 2006 CUDA redesign made this easier by removing graphics-specific constraints, but the parallel matrix multiplication capability was inherent to graphics architecture from the beginning.

## Understanding linear algebra is fundamental to understanding AI

Neural networks are fundamentally systems of matrix operations, making linear algebra the essential mathematical language for comprehending how AI actually works.

**All data in machine learning exists as tensors**—multidimensional arrays represented mathematically as matrices and vectors. Images are 3D tensors (height × width × channels), text embeddings are vectors and matrices, and neural network weights are matrices. Every layer in a neural network essentially performs the operation: `output = activation(W × input + bias)`, where W is a weight matrix and the core computation is matrix multiplication.

Forward propagation through a neural network consists of chained matrix-vector and matrix-matrix products. Backpropagation applies the chain rule to matrices for gradient calculations. Attention mechanisms—the foundation of modern transformers—rely on queries, keys, and values that are all linear transformations expressed through matrix operations. Even convolutions, despite their different conceptual framing, can be reformulated as matrix multiplications.

You can understand AI **conceptually** without linear algebra—grasping that neural networks transform data and learn patterns—but you cannot understand **how** the transformations work, why certain architectures prove effective, or how to debug and optimize models. Understanding why operations are computationally expensive, how dimensionality reduction works, or how to design efficient architectures all require solid linear algebra foundations. Machine learning textbooks consistently identify linear algebra as "the mathematical foundation that solves the problem of representing data as well as computations in machine learning models."

## CPUs and GPUs differ in parallelism optimization, not just focus

CPUs are latency-optimized processors for task parallelism with complex control flow, while GPUs are throughput-optimized processors for data parallelism with simple control flow.

Modern CPUs contain 2-192 cores (AMD EPYC 9005 Series), where each core is a sophisticated independent processing unit. CPUs allocate transistors to complex control logic—branch predictors achieving 95-99% accuracy, out-of-order execution units, large multi-level caches (L3 caches up to 256 MB), and speculative execution capabilities. The Intel P6 architecture's Reorder Buffer tracks 126 micro-operations in flight, while modern designs handle 224-512 entries. CPUs employ **superscalar execution** issuing 3-6 micro-ops per clock cycle, maintain cache coherency across cores, and support complex control flow with nested branches, loops, and function calls.

**CPUs excel at task parallelism**—running different programs with different instructions simultaneously across cores. Operating systems schedule heterogeneous workloads (web browsers, video encoding, system processes) across CPU cores. Each core can execute entirely different code paths, making CPUs ideal for branch-heavy, control-intensive workloads with irregular memory access patterns.

Modern GPUs contain **thousands to tens of thousands of cores** (NVIDIA RTX 4090: 16,384 CUDA cores; H100: 18,432 cores) organized into Streaming Multiprocessors. GPUs allocate a large percentage of transistors to actual computation (ALUs/FPUs) rather than control logic. They use **SIMT execution (Single Instruction, Multiple Threads)** where threads are organized into warps (NVIDIA: 32 threads) or wavefronts (AMD: 64 threads), with one control unit per warp shared across 32-128+ cores.

GPUs employ minimal branch prediction per core, have smaller caches relative to computational power (16-48 KB L1 per SM), but feature dramatically higher memory bandwidth (1-3+ TB/s vs CPU's 100-200 GB/s). They hide memory latency through **massive parallelism**—when one warp stalls on memory access, another immediately executes, a technique called latency hiding. GPUs perform in-order execution per warp but achieve parallelism by running thousands of warps concurrently.

**Both CPU and GPU architectures handle PARALLEL workloads but in fundamentally different ways.** CPUs multi-task and optimize for task parallelism where different cores run different programs; GPUs optimize for data parallelism where thousands of threads execute identical operations on different data elements. This makes CPUs superior for human workflows that typically involve irregular, branch-heavy workloads with complex control flow, while GPUs excel at regular, but very compute-intensive operations like matrix multiplication where the same operation applies to millions of independent data elements.

## GPUs revolutionized AI through architectural accident, not design

Although this was known by scientifc compute specialists before, GPUs became central to artificial intelligence through a fortunate architectural coincidence discovered BY THE PUBLIC [including venture capitalists] long after their creation. In 2003, two independent research groups discovered that GPU-based approaches for linear algebra ran faster than CPUs, but this required awkward workarounds reformulating problems as graphics operations. Matrix manipulations, linear algebra and scientific compute was thoroughly ghetto-ized, punted deep into in the nerd realm of EXPENSIVE supercomputers and computing that was not just outside, but FAR, FAR, FAR outside the budget of all but the very most well-funded research programs. When the timing was finally right and the public and audience of VCs were FINALLY ready to understand how scientific compute might become democratized or available to more serious data scientists and people in industry, eg SixSigma BlackBelts, it was a fairly simple matter for scientific compute experts to come up with examples to demonstrate some practical examples of a much larger role for GPUs.  In 2006, researcher K. Chellapilla trained a convolutional neural network on a GPU achieving 4× speedup over CPUs—an early demonstration of untapped potential. 

{**RULE: Understanding technology is COMPLETELY DIFFERENT than understanding public adoption.** Curtis Priem, a co-founder of Nvidia, began selling his shares in 1999, the year the company went public, had sold off his entire stake by 2006. Priem is sometimes forgotten as cofounder of Nvidia, mostly due to his early retirement, stock sellout, and move off the grid but Priem is a remarkable inventor with almost 200 U.S. and international patents as well as a philanthropist who has donated hundreds of millions of his fortune to causes he cares about ... he has indicated that he wished that he had hung on to the shares of nVidia, but only because it would provide more resources to his philanthropies/causes ... *perhaps the investors who've earn significant wealth with their nVidia shares will use it to fund their causes*. As we all know or SHOULD know, Priem's lifestyle would not change in a positive way by having more money -- so he made the right choice!.}

**NVIDIA's release of CUDA in November 2006** provided the crucial infrastructure, allowing programmers to use GPUs for general computation without graphics programming knowledge. Early neural network experiments in 2010-2011 by Jürgen Schmidhuber's team and Andrew Ng demonstrated 50× speedups for training deep neural networks, but the field remained niche.

The breakthrough came **September 30, 2012, when AlexNet won the ImageNet challenge**. Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, this convolutional neural network trained on two NVIDIA GTX 580 GPUs in Krizhevsky's bedroom, achieving 15.3% error rate compared to 26.2% for second place—a 41% relative improvement. Yann LeCun called it "an unequivocal turning point in the history of computer vision."

Fei-Fei Li identified that three fundamental elements converged for the first time in 2012: neural networks (algorithmic innovation), big data (ImageNet dataset), and GPU computing (hardware acceleration). This convergence worked because **matrix multiplication is inherently parallel**—each element in an output matrix can be computed independently, allowing GPUs to calculate thousands of matrix elements simultaneously. A 4096×4096 matrix multiplication demonstrated 593× speedup over sequential CPU processing and 45× over parallel CPU execution.

NVIDIA subsequently introduced **Tensor Cores in the Volta architecture (2017)**—specialized units for matrix multiply-accumulate operations that further optimized GPUs for AI workloads. Fifth-generation Tensor Cores can perform 256×256×16 matrix operations in a single instruction. These were added **after** GPUs became popular for AI, representing a deliberate optimization to serve the emerging larger market of ML/AI, rather than GPUs original design for gaming speed/performance. NVIDIA's market capitalization grew from under $10 billion in 2012 to over $3 trillion by 2024, driven almost entirely by this *discovery* by the investing public [after 2012, AlexNet and the "ImageNet Challenge" notoriety of work by Krizhevsky, Sutskever, Hinton, et al], ***financially*** motivating the scramble for a rather radical change in the hardware focus of the techology building more upon CUDA and additional enabling steps, eg high-bandwidth memory,Infiniband interconnect technology.

## Conclusion: Refining the historical narrative

**Timeline**: EARLY graphics display processors, which could be labeled as GPUs, existed in 1982 for professional CAD applications, but truly modern GPUs were actually born 1996-1999 with gaming as the primary driver. The term "GPU" was coined by Sony in 1994 [using *vaporware* in their terminology for marketing campaigns, to build demand for innovative technology, that was imagineed, but not delivered or even yet being manufactured as prototypes]. Based on the success of Sony's marketing hype, the "GPU" term was popularized by NVIDIA in 1999.

**Purpose**: The community that developed consumer GPUs came from scientific computing and workstation graphics backgrounds where matrix manipulation was foundational knowledge. 3D graphics rendering inherently requires linear algebra (transformations, rotations, projections). Gaming didn't invent the need for matrix operations—it created the mass market that made consumer-priced matrix manipulation hardware economically viable. The knowledge and mathematical foundations existed in the workstation community first; gaming and the gaming community democratized access to specialized workstation technology.

**Architecture**: The CPU/GPU distinction is not exactly "multitasking vs focused" but rather latency-optimized multi-tasking parallelism versus throughput-optimized data parallelism. Both multi-tasking computers and focused scientific workstations were/are parallel processors optimized for different parallelism types. The Intel 80486 was a CPU released in 1989 with no relationship to GPU development. Although different technical writers have different experience and insights on this matter, they should attempt to distinguish between CPUs (general-purpose processors), graphics processors (early 1980s specialized chips), 3D accelerators (mid-1990s gaming hardware), and modern GPUs (1999+ unified graphics processors later adapted for general computing).

**Linear algebra**: Linear algebra is fundamental to understanding AI. Neural networks are systems of matrix operations, making linear algebra the essential mathematical framework rather than optional background knowledge.

